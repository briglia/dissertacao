\documentclass[12pt,brazil,a4paper]{report}
\usepackage[top=3cm,left=3cm,right=2cm,bottom=2cm]{geometry}
%\usepackage{sbc2003}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{boxedminipage}
%\usepackage{epsf}
\usepackage[latin1]{inputenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{setspace}

\usepackage{float}
\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Algoritmo}
\renewcommand{\theprogram}{\thechapter.\arabic{program}}

\newcommand{\tittese}{%
	\textsf{\bfseries\LARGE Cache Comprimido Adaptativo \\
	\vspace{0.8ex}
	via SOM (Mapas Auto-organizáveis)}
}

\newcommand{\descrtese}{%
\hspace{\stretch{1}}\parbox{0.51\textwidth}{%
Dissertação apresentada ao Programa de Pós-Graduação em Informática do Departamento de Ciência 
da Computação da Universidade Federal do Amazonas, como requisito para obtenção do 
Título de Mestre em Informática.
}}

\begin{document} 

% -- Capa ---------------------------------------------------------------------

{\singlespacing\centering
\includegraphics[bb=0 0 646 638,height=2.5cm]{figs/ufam.png}

\textsf{\large%
Universidade Federal do Amazonas\\
Instituto de Ciências Exatas\\
Departamento de Ciência da Computação\\
Programa de Pós-Graduação em Informática
}

\vspace*{\stretch{1}}

\tittese

\vspace*{\stretch{1}}

{\large Anderson Farias Briglia}

\vspace*{\stretch{1}}

Manaus -- Amazonas

2009

}
\cleardoublepage


% -- Contracapa ---------------------------------------------------------------

{\singlespacing\centering
{\large Anderson Farias Briglia}

\vspace*{\stretch{2}}

\tittese

\vspace*{\stretch{1}}

\descrtese

\vspace*{\stretch{1}}

Orientador: Dr. Edward Moreno

}
\cleardoublepage


% -- Banca Examinadora --------------------------------------------------------

{\singlespacing\centering

{\large Anderson Farias Briglia}

\vspace*{\stretch{2}}

\tittese

\vspace*{\stretch{1}}

\descrtese

\vspace*{\stretch{1}}

Banca Examinadora
\vspace{1em}

Prof. Edward David Moreno Ordonez, Ph.D. -- Orientador \\
Departamento de Ciência da Computação -- UFS/DCOMP
\vspace{1em}

Prof. Dr. Raimundo da Silva Barreto, Ph.D. -- Co-Orientador \\
Departamento de Ciência da Computação -- UFAM/PPGI
\vspace{1em}

Prof. Carlos Alberto Estombelo, Ph. D. -- Banca \\
IFAM - Manaus


\vspace*{\stretch{0.5}}

Manaus -- Amazonas

2009

}
\cleardoublepage


% -- Dedicatória --------------------------------------------------------------

\vspace*{\stretch{2}}

\hspace{\stretch{1}}\textit{Aos meus pais Francisco e Irene.}\hspace{1cm}

\vspace*{\stretch{1}}

\cleardoublepage

% -- Agradecimentos -----------------------------------------------------------

\chapter*{Agradecimentos}
\thispagestyle{empty}

{\setlength{\parindent}{0cm}

Aos meus pais, por sempre acreditarem que o estudo é o melhor caminho para o crescimento.

A minha esposa, Mariana Paraguassu, pela alegria, carinho e total apoio nas horas mais turbulentas durante o curso de mestrado.

Aos meus orientadores, Edward Moreno e Raimundo Barreto, por me ajudarem a terminar esta dissertação.

Ao meu ex-chefe e colega, Ilias Biris, pelo incentivo de ingressar no mestrado e orientação de como iniciar o trabalho.

Ao colega indiano Nitin Gupta, por ter disponibilizado o projeto de Cache Comprimido na Internet, o qual foi baseado esta dissertação.

Aos meus colegas de trabalho Maurício Lin e Francisco Alecrim por me darem total apoio ao usar o trabalho deles afim de auxiliar o desenvolvimento desta dissertação.

A todas as pessoas que me ajudaram de alguma forma na conclusão deste trabalho, muito obrigado.

}

\cleardoublepage


% -- Epígrafe -----------------------------------------------------------------

{\singlespacing
\vspace*{\stretch{1}}

\hspace{\stretch{1}}\parbox{0.5\textwidth}{%
Só é útil o conhecimento que nos torna melhores.
}
\vspace{2em}

\hspace{\stretch{1}}{\emph{Sócrates}}

\vspace*{\stretch{1}}
}
\cleardoublepage


% -- Resumo -------------------------------------------------------------------

\chapter*{Resumo} 
\thispagestyle{empty}

{\onehalfspacing

Sistemas embarcados geralmente possuem limitações de memória e processamento. Com o aumento do uso dos dispositivos móveis é mais comum encontrar sistemas operacionais embarcados, como o Linux, sendo usados para executar aplicações multimídia, acessar a Internet, etc. Aumentar a memória física de um dispositivo pode ser custoso e após projetado é inviável de ser ter mudanças físicas no dispositivo, por esse motivo implementar mecanismos via software pode ser uma possível solução mais acessível e econômica à maioria dos usuários.

O Cache Comprimido é uma técnica que permite comprimir a memória das aplicações, possibilitando um aumento da memória disponível sem a necessidado de trocar o hardware. Porém, a falta de um Cache Comprimido Adaptativo faz com que essa solução tenha um escopo reduzido de benefícios.

Nesta dissertação foi desenvolvido um programa que utiliza a classificação de padrões de consumo de memória usando (SOM - Self Organized Maps -- Mapas Auto-Organizáveis), para implementar um Cache Comprimido Adaptativo, que consiga se adequar ao consumo da memória em tempo de execução.

\vspace*{\stretch{1}} %texto se adapta para ocupar toda a página

\noindent \textsf{Palavras-chave:} Linux, Gerenciamento de Memória, Classificação de Padrões e Redes Neurais, Cache Comprimido.

} %onehalfspacing


\cleardoublepage


% -- Abstract -----------------------------------------------------------------

\chapter*{Abstract}
\thispagestyle{empty}

{\onehalfspacing

Embedded systems usually have memory and processing limitations. Due the growth of mobile devices usage, it is common to see embedded systems being used to run multimedia applications, access the Internet, etc. Increase the mobile physical memory amount could be expensive and after the hardware design it becomes undoable, due this implement a software solution could be more accessible in terms of costs. 

Compresse Cache is a solution which provides applications memory compression, improving a increasement in available memory without using hardware changes. But, the lack of an Adaptive Compressed Cache implementation makes this solution's scope smaller in terms of benefits.

A program which uses memory consumption patterns through SOM -- Self Organized Maps was developed in this dissertation, to implement an Adaptive Compressed Cache. This approach makes Cache Compression behavior aligned with memory consumption in real time. 

} %ondehalfspacing

\cleardoublepage


\tableofcontents
\listoffigures
\listoftables

\newpage

\onehalfspacing

\chapter{Introdução}
%\section {Introdução}
\label{sec:introducao}
A grande maioria dos sistemas embarcados têm como principal característica seu uso para uma tarefa específica, porém, existem sistemas embarcados que não necessariamente são específicos \cite{mor00}. Como é o caso do sistema operacional \textit{Linux} analisado nesta dissertação de mestrado. O avanço da minituarização dos componentes eletrônicos e a queda do preço dos equipamentos, fez com que o poder de processamento de um PDA (\textit{Personal Data Assistance}), por exemplo, caísse consideravelmente, permitindo que esse tipo de dispositivo possa ter sistemas embarcados mais sofisticados.\\

Com o poder de processamento dos dispositivos móveis aumentando, as aplicações disponíveis aos usuários acompanham este crescimento e se tornam mais complexas. Como características inerentes de qualquer sistema embarcado, pode-se citar o processamento limitado, consumo de energia bem restrito e problemas relacionados à memória disponível para as aplicações. Nesta dissertação, uma possível solução para o problema de memória dos sistemas embarcados é proposta e analisada. Possíveis soluções para o problema de consumo de potência ou aumento do poder de processamento dos sistemas embarcados, não é o foco deste trabalho.\\

Segundo \cite{castro03}, uma das possíveis soluções para o problema de escassez de memória nos sistemas embarcados é a utilização de algoritmos de compressão. A compressão tem se mostrado uma técnica eficiente para otimizar o uso da memória em sistemas embarcados\cite{briglia07, castro03, douglis93, irina05, kaplan99compressed}. Ela também tem sido utilizada como um meio de melhorar o uso da memória cache, reduzindo assim o consumo de potência e melhorando a performance do sistema de memória, visto que a memória cache é, geralmente, mais rápida do que a memória principal de um computador ou de um dispositivo móvel. E sendo uma memória de acesso rápido, o processador gasta menos tempo para acessá-la, beneficiando também o consumo de potência total do sistema.\\

O \textit{Cache} Comprimido (CC) é uma técnica que adiciona um novo nível na hierarquia de memória do Linux \cite{castro03, douglis93, kaplan99compressed}. O CC é usado para aprimorar o tempo de acesso às páginas de memória no \textit{kernel} (ou núcleo) do Linux, armazenando mais páginas na memória RAM (\textit{Random Access Memory}), e reduzindo o número de páginas que vão para a área de \textit{swap}\footnote{Área de swap é uma área reservada no sistema de arquivos do Sistema Operacional, afim de ser utilizada como uma extensão de memória principal.} ou que seriam descartadas caso o sistema não a possuísse. É sabido que a área de \textit{swap}, em geral, é muito mais lenta que a memória principal, e custosa com relação ao consumo de energia pois na maioria dos casos está associada a dispositivos de bloco, como por exemplo, um disco rígido. E ainda tem-se o problema de que em sistemas embarcados geralmente essa área não está presente ou n\~{a}o possui o tamanho ideal.\\

Na implementação usada neste trabalho, o tamanho da área de CC possui influência na quantidade de memória disponível para as aplicações. Se o tamanho do CC, ou seja, da memória alocada para ele, for muito grande, podem ocorrer situações de falta de memória, ocasionando em travamento do sistema ou das aplicações que estão sendo executadas no momento. Hoje, a escolha de um tamanho para o CC é empírica. Ou seja, é escolhido um tamanho baseado em experimentos (como os vistos nas seções posteriores deste trabalho), ou simplesmente na estimativa de memória total livre após a alocação do CC. Um cenário ideal para calcular o tamanho do CC é que o perfil das aplicações, ou melhor, das alocações de memória das aplicações, seja levando em conta quando o tamanho for escolhido.\\

Como forma de estimar o comportamento da memória, alinhando assim o tamanho do CC a ser utilizado com a forma de como as aplicações utilizam a memória, é utilizado o trabalho de mestrado de Maurício Lin \cite{Alecrim:07, Mlin:06}. Neste trabalho foi implementado um esquema de classificação de padrões de consumo de memória utilizando Mapas Auto-Organizáveis (do inglês SOM: \textit{Self-Organized Maps}). Basicamente é feito um ``mapa'' de como a memória foi utilizada por determinada aplicação. Nesta dissertação de mestrado, esse mapa é utilizado como entrada para a ferramenta que irá calcular o tamanho do CC, baseado no perfil das aplicações testadas.

\section {Motivação}
\label{sec:motivacao}

A memória é um dos componentes críticos que possuem maiores restrições quando usada em sistemas embarcados. Por outro lado, os sistemas têm sido aperfeiçoados através de técnicas sofisticadas, algoritmos complexos e suporte a tempo-real. Como consequência, as aplicações para sistemas embarcados têm se tornado maiores e com um volume de dados manipulados sempre crescente.\\

Dado esse cenário, é muito importante definir mecanismos que aperfeiçoem a utilização de memória e/ou a performance das aplicações quando estas fazem uso intenso da memória do dispositivo.\\

Em \cite{castro03}, foi implementada uma versão do Cache Comprimido Adaptativo para a versão 2.4.x do \textit{kernel} do Linux. Os mecanismos de falta de memória encontrados na versão 2.4.x são diferentes do que temos hoje (ex. mapeamento de páginas anônimas, Out-of-memory killer, etc), nas versões mais atuais (2.6.x, por exemplo).\\

Um ponto motivacional deste trabalho é a realização de uma implementação do Cache Comprimido Adaptativo para as versões mais atuais do \textit{kernel} do Linux, disponibilizando assim seu uso em dispositivos móveis mais atuais, baseados neste sistema operacional. A implementação usada neste trabalho está em desenvolvimento \cite{ccache09}, e a idéia desta dissertação é contribuir nesse projeto \textit{Open Source}, além de melhorar o desempenho das aplicações quando executadas em ambiente de Linux embarcado, propondo uma nova metodologia de gerenciamento de memória no \textit{kernel} do Linux.\\

Na versão do Cache Comprimido Adaptativo apresentada em \cite{ccache09}, não foi utilizada nenhuma técnica para estimar o comportamento do tamanho da memória comprimida. A escolha de uma heurística inadequada pode impactar no desempenho de todo o sistema, pois a relação memória comprimida X memória não-comprimida é muito importante quando se trata de adaptatividade. Estudos realizados anteriormente em \cite{castro03}, mostram que uma escolha errada no tamanho da memória comprimida pode criar \textit{overheads} desnecessários ao sistema, acarretando em perda de performance. Assim, a implementação de um esquema de adaptatividade utilizando SOM se torna um outro ponto de motivação do trabalho.\\

Por último, e não menos importante, um outro ponto de destaque da motivação deste trabalho é que não foram feitos testes na arquitetura utilizada aqui. Testes com a última versão do Cache Comprimido foram feitos em versões do Linux para desktops e netbooks, mas ainda não foram realizadas medições em um ambiente de Linux embarcado para arquitetura ARM.

\section{Objetivos}
\label{sec:objetivos}

O propósito principal deste trabalho é desenvolver um sistema de memória comprimida, utilizando como base a implementação \textit{Open Source} encontrada em \cite{ccache09}, e que implemente o conceito de adaptatividade do tamanho do CC, baseado em perfis de consumo de memória das aplicações, definidos utilizando uma rede neural e SOMs (mapas auto-organizáveis).\\

Experimentos realizados em trabalhos anteriores \cite{briglia07, castro03}, indicaram que o tamanho da área comprimida de memória afeta o desempenho do sistema. Espera-se que, seja possível classificar o uso da memória, como mostrado em \cite{Mlin:06} e utilizar esses dados na heurística de como a área de cache comprimido deve ser dimensionada. A implementação deste trabalho não possui dependências de arquitetura, apesar de ser focada em Linux embarcado para arquitetura ARM (\textit{Advanced RISC Machine}).\\

A área para memória comprimida será adicionada ao sistema existente como mostrado na Figura \ref{fig:cc_intro}.

\begin{figure}[ht]%htbp
\centering
\includegraphics[scale=0.85]{figs/cc-intro}
\caption{Hierarquia de memória com cache comprimido}
\label{fig:cc_intro}
\end{figure}

A versão do \textit{kernel} utilizada é a 2.6.x, e como resultado prático final um \textit{patch} ou uma série de \textit{patches} serão gerados, com modificações relativas à implementação do CC para a última versão do \textit{kernel} disponível. Uma aplicação que extrai os dados providenciados pelo SOM durante a classificação do uso da memória, será implementada. Essa aplicação exportará os dados necessários (como os perfis de consumo de memória de aplicações escolhidas), para o ajuste do tamanho do CC. Na Seção \ref{sec:classe_rede} é apresentada a forma de como os padrões de memória são classificados.\\

Para a validação da implementação são utilizados dois tipos de testes preliminares: \textit{benchmarks} sintéticos e testes com casos de uso que simulam a utilização real do sistema, utilizando aplicações gráficas tais como: navegador \textit{Web}, leitor de arquivos PDF, tocador de áudio e vídeo. Nos testes com \textit{benchmarks} sintéticos, foram utilizados suites como o MemTest \cite{memtest06} e o MiBench \cite{GRE01}. O MemTest foi utilizado nos testes de performance pois sua principal característica é a realização de operações com a memória virtual do Sistema Operacional Linux, mais especificamente, com alocações e desalocações de porções de memória. O MiBench é bastante utilizado como \textit{benchmarking} de sistemas embarcados e provê um conjunto de módulos que podem ser utilizados como parâmetros de referência tanto para medidas relacionadas à arquitetura de \textit{hardware} quanto a medidas com operações de memória, ou operações multimídia. Basicamente os testes usando essas ferramentas fazem um uso intenso da memória RAM. No caso dos testes utilizando casos de uso, é utilizada uma ferramenta de automação chamada XAutomation \cite{xauto07}, para criar uma interação com o ambiente gráfico do sistema, simulando um uso real das aplicações mencionadas anteriormente, enquanto dados referentes à utilização da memória livre são armazenados. Com estes dois tipos de testes, espera-se ter dados suficientes para apontar o impacto da utilização de compressão da memória utilizando SOM, com a heurística para adaptar o tamanho da memória comprimida.

%\section{Cronograma}
%\label{sec:cronograma}
%O trabalho aqui proposto é dividido em etapas distintas conforme detalhadas nos itens seguintes. O cronograma apresentado possui informações para as etapas passadas e futuras do projeto.\\\\

%\textbf{Cronograma de 2007:}
%\begin{enumerate}
%\item Levantamento Bibliográfico - Coleta, organização e análise da literatura técnica relacionada com os assuntos abordados no trabalho.
%\item Implementação do Cache Comprimido sem adaptatividade - Implementação do Cache Comprimido para Linux embarcado \cite{ccache09}.
%\item Testes com o CC atual - Preparação do dispositivo móvel e da suite de testes.
%\item Artigo para OLS2007 - Escrita do artigo com detalhes de implementação e dos testes para o Linux Symposium 2007.
%\item Preparação da proposta - Revisão da bibliografia e dos trabalhos relacionados.
%\end{enumerate}
% 
% \vspace{\baselineskip}
% 
% \newlength{\constlen}
% \newlength{\pllen}
% \setlength{\constlen}{7mm}
% \newcommand{\mkp}[1]{\makebox[\constlen]{#1}}
% 
% \newcommand{\putobj}[3]{
% \setlength{\unitlength}{1mm}
% \begin{picture}(0,0)
% \put(#1,#2){#3}
% \end{picture}}
% \newcommand{\pl}[1]{\setlength{\pllen}{#1\constlen}\putobj{0}{0}{\rule[0.3mm]{\pllen}{2mm}}}
% 
% \setlength{\tabcolsep}{0pt} %largura entre separadores das colunas
% \renewcommand{\arraystretch}{1.25} %altura das colunas
% 
% \begin{table}[ht]
%     \centering
% \begin{tabular}{|l|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|}
% \hline
% \makebox[5cm]{Tarefas} &
% \multicolumn{1}{c|}{Jan} &
% \multicolumn{1}{c|}{Fev} &
% \multicolumn{1}{c|}{Mar} &
% \multicolumn{1}{c|}{Abr} &
% \multicolumn{1}{c|}{Mai} &
% \multicolumn{1}{c|}{Jun} &
% \multicolumn{1}{c|}{Jul} &
% \multicolumn{1}{c|}{Ago} &
% \multicolumn{1}{c|}{Set} &
% \multicolumn{1}{c|}{Out} &
% \multicolumn{1}{c|}{Nov} &
% \multicolumn{1}{c|}{Dez} \\
% \hline
% \hline
% Levantamento Bibliográfico &\pl{5} & & & & & & & &\pl{3} & & &  \\
% \hline
% Implementação do CC s/ adapt. & & & &\pl{5} & & & & & & & &\\
% \hline
% Testes com o CC atual & & &\pl{3} & & & & & & & & & \\
% \hline
% Artigo para OLS2007 & & &\pl{4} & & & & & & & & & \\
% \hline
% Preparação da proposta & & & & & & & & & & &\pl{2} &\\
% \hline
% \end{tabular}
%     \caption{Cronograma de trabalho em 2007.}
%     \label{tab:cronograma}
% \end{table}
% 
% \textbf{Cronograma para 2008:}
% \begin{enumerate}
% \item Preparação da proposta - Revisão da bibliografia, dos trabalhos relacionados e escrita da proposta.
% \item Testes com SOM - Baseado em \cite{Alecrim:07, Mlin:06}, fazer os testes e obter os primeiros resultados utilizando a última versão do Linux embarcado para o dispositivo móvel alvo.
% \item Avaliação dos testes com SOM - Verificar os perfis de consumo de memória coletados no item anterior.
% \item Implementação do CC com adaptatividade - Implementação de modificações na versão atual do CC para a utilização de adaptatividade.
% \item Integração SOM e CC - Implementação de uma ferramenta que avalie os mapas resultantes da avaliação dos vários perfis de memória, e o Cache Comprimido. A idéia é utilizar a saída dessa ferramenta como entrada para a heurística de adaptatividade do CC.
% \item Testes Finais - Testes com a versão do CC com adaptatividade utilizando SOMs.
% \item Confecção da dissertação - Escrita da dissertação de mestrado.
% \item Revisão Final - Revisão da implementação e da dissertação.
% \end{enumerate}
% 
% \begin{table}[ht]
%     \centering
% \begin{tabular}{|l|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|p{\constlen}|}
% \hline
% \makebox[5cm]{Tarefas} &
% \multicolumn{1}{c|}{Jan} &
% \multicolumn{1}{c|}{Fev} &
% \multicolumn{1}{c|}{Mar} &
% \multicolumn{1}{c|}{Abr} &
% \multicolumn{1}{c|}{Mai} &
% \multicolumn{1}{c|}{Jun} &
% \multicolumn{1}{c|}{Jul} &
% \multicolumn{1}{c|}{Ago} &
% \multicolumn{1}{c|}{Set} &
% \multicolumn{1}{c|}{Out} &
% \multicolumn{1}{c|}{Nov} &
% \multicolumn{1}{c|}{Dez} \\
% \hline
% \hline
% Preparação da Proposta &\pl{2} & & & & & & & & & & &  \\
% \hline
% Testes com SOM & & &\pl{2} & & & & & & & & &\\
% \hline
% Avaliação dos testes & & &\pl{1} & & & & & & & & & \\
% \hline
% Implementação do CC com adaptatividade & & & &\pl{3} & & & & & & & & \\
% \hline
% Integração SOM e CC & & & & &\pl{3} & & & & & & &\\
% \hline
% Testes Finais & & & & & &\pl{2} & & & & & &\\
% \hline
% Confecção da dissertação & & & & & & &\pl{4} & & & & &\\
% \hline
% Revisão Final & & & & & & & & & & &\pl{2} &\\
% \hline
% \end{tabular}
%     \caption{Cronograma de trabalho para 2008.}
%     \label{tab:cronograma}
% \end{table}

\section{Organização da dissertação}

O Capítulo 1 trata da introdução, motivação e objetivos desse trabalho. Neste capítulo o leitor será apresentado ao problema e à solução proposta pelo autor.\\

O Capítulo 2 descreve o estado atual da implementação do Cache Comprimido utilizada neste trabalho. Detalhes de implementação e testes também são descritos e discutidos.\\

O Capítulo 3 apresenta o trabalho realizado em \cite{Alecrim:07} e \cite{Mlin:06}. Estes trabalhos propõem uma classificação de padrões de consumo de memória utilizando redes neurais e mapas auto-organizáveis.\\

No Capítulo 4 o leitor terá acesso aos resultados obtidos com os testes utilizando Cache Comprimido e Mapas Auto-organizáveis. Também será apresentada a solução encontrada para implementar a adaptatividade do Cache Comprimido quando usado em Linux embarcado.\\

No Capítulo 5 é apresentado os trabalhos futuros para o Cache Comprimido e a conclusão encontrada após a relização dos testes e implementação envolvida.

\chapter{Cache Comprimido}

Esta seção tem o propósito de apresentar os estudos e as pesquisas realizadas até o momento e que são requisitos relevantes para o desenvolvimento da proposta descrita na Seção \ref{sec:objetivos}.\\

Inicialmente é apresentado o estado da arte, onde alguns trabalhos relacionados são analisados. Em seguida, a atual implementação do CC é discutida, para a versão 2.6.x do \textit{kernel} do Linux encontrada em \cite{ccache09}. Também são exibidos e descritos alguns testes realizados com essa versão e apresentados em \cite{briglia07}.

\section{A memória virtual do Linux}

Páginas físicas são a unidade básica do gerenciamento de memória \cite{love05kerneldevel} e o MMU (\textit{Memory Management Unit} é o hardware responsável por traduzir endereços virtuais em reais das páginas de memória, e vice-versa. \\

No gerenciamento da memória virtual, duas listas do tipo LRU (\textit{Last Recently Used}) são utilizadas afim de classificar as páginas: LRU para páginas ativas e uma LRU para páginas inativas. Quando o sistema precisa alocar novas páginas, elas são inicialmente retiradas da lista LRU de páginas inativas. O algoritmo responsável por selecionar e liberar as páginas é chamado de \textit{Page Frame Reclaiming Algorithm} - PFRA.\\

Afim de identificar cada tipo de página, \textit{flags} são utilizadas na estrutura de dados que as implementam. Para diferenciar as páginas comprimidas das páginas comuns, a implementação atual do Cache Comprimido adiciona uma \textit{flag} na estrutura de dados da página.\\

Quando o sistema está sob pressão de memória, ou seja, há menos memória disponível que o necessário, o PFRA libera as páginas de acordo com a sua classificação:

\begin{itemize}
\item Páginas do \textit{Swap-cache} são escritas na área de \textit{swap} disponível.

\item Páginas "sujas" do \textit{Page cache} são escritas no \textit{filesystem} utilizando o procedimento específico de escrita.

\item Páginas "limpas" do \textit{Page Cache} são simplesmente liberadas da memória, e ficam disponíveis para serem utilizadas novamente por outros processos.
\end{itemize}

\subsection{O Swap Cache}

Este é o cache para páginas anônimas. Toda as páginas do \textit{swap cache} são parte de um único \textbf{swapper\_space}, a estrutura que agrupa todas as páginas que podem ir para a área de \textit{swap}. Uma outra estrutura de dados, chamada \textit{radix tree} é utilizada para manter todas as páginas do \textit{Swap cache} e torna a busca por páginas muito mais eficiente. Essa estrutura de dados é a implementação de uma árvore-b em que cada nodo possui um vetor de 64 posições que apontam para o endereço virtual da página. Quando uma página sai do Swap Cache e vai para a área de swap, esse endereço é atualizado e é assim que o kernel do Linux consegue recuperar uma página quando é requisitada.

O campo \textbf{swp\_entry\_t} da estrutura de dados da área de \textit{swap} é utilizado como chave-de-busca quando uma página do \textit{swap cache} é procurada. Esta estrutura é usada para identificar onde a página requisitada se encontra no dispositivo de bloco utilizado pelo seu \textit{swap}.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.8]{figs/swp_entry}
	\caption{Campos da estrutura \texttt{swp\_entry\_t}}
	\label{fig:swp_entry_fields}
\end{figure}

Na Figura \ref{fig:swp_entry_fields}, \textbf{`type'} identifica em qual área de \textit{swap} a página de se encontra. No sistema operacional Linux, podem haver até 32 \textit{swaps} simultâneos.

\subsection{O Page Cache}
Este é o cache utilizado para armazenar as páginas do \textit{filesystem}. Ou seja, todo arquivo aberto pelas aplicações possui páginas de memória alocadas e armazenadas no \textit{Page Cache}. Assim como o \textit{Swap Cache}, este \textit{cache} também possui uma \textit{radix tree} que armazena as referências, ou melhor, os ponteiros para as páginas de um determinado arquivo presente no sistemas de arquivos, que esteja sendo utilizado. O valor de \textit{offset} dentro do arquivo é utilizado como chave-de-busca para localizar tais páginas. Cada arquivo aberto possui uma \textit{radix tree} própria, com os nodos da árvore apontando para as páginas pertencentes ao arquivo.

\section{Cache Comprimido}
Em um sistema com cache comprimido, a memória é dividida em duas grandes porções: memória comprimida e não-comprimida \cite{castro03-2, douglis93, kaplan99compressed}. A área de memória comprimida geralmente é alocada onde antes existia memória não-comprimida. A relação dos tamanhos da memória comprimida e não-comprimida deve ser avaliada pois os dois tamanhos interferem em como cada porção de memória se comporta, dependendo do \textit{workload} imposto pelas aplicações.\\

Muitos pesquisadores \cite{kaplan99compressed, irina05} têm investigado o uso de compressão para reduzir as operações de \textit{paging}, introduzindo um novo nível na hierarquia de memória. Armazenar as páginas de memória em uma área comprimida, naturalmente aumenta o tamanho efetivo da memória e também diminui o acesso à dispositivos de memória secundária \cite{castro03}, que é muito mais lenta que a principal. Apesar dessa diferença de velocidade entre a memória principal e a memória secundária ser grande, quando se leva em conta os sistemas embarcados, ela é menor. Geralmente, os sistemas embarcados não possuem memória secundária armazenada em um disco rígido, outros dispositivos são utilizados nesse caso. Memórias do tipo \textit{compact flash} e cartões MMC, são os principais dispositivos encontrados hoje no mercado que são utilizados como memória secundária em dispositivos móveis \footnote{A velocidade de leitura de um cartão MMC comum, chega à taxa de 416 Mbits/seg \cite{mmca}. A velocidade das memórias RAM's variam, mas as mais comuns possuem um tempo de acesso de 80-90 ns.}. Mesmo tendo a diferença de velocidade de acesso entre a memória principal e a secundária menor, os sistemas embarcados possuem um grande requisito relacionado ao tamanho dessa memória. Assim, o uso de compressão é justificado pois, como será discutido posteriormente, é capaz de aumentar o tamanho efetivo da memória disponível para as aplicações.\\

As abordagens de cache comprimido baseadas em \textit{software}, ou seja, aquelas que não propõem alteração de \textit{hardware}, podem ser estáticas ou dinâmicas. As abordagens estáticas \cite{swapcc99, wilson99, crames05} são caracterizadas por não possuírem uma heurística de redimensionamento do cache comprimido, diferente das abordagens dinâmicas. Nesse segundo tipo, existe um algoritmo que determina quando o cache comprimido deve alterar seu próprio tamanho, geralmente baseado no \textit{workload} da memória.\\

Os primeiros estudos que utilizavam cache comprimido para reduzir a paginação em disco, ou seja, a quantidade de acessos de leitura/escrita, foram feitos por Appel e Li \cite{li91} e Paul R. Wilson \cite{wilson91}, em 1991. Outro pesquisador, chamado Douglis\cite{douglis93}, obteve algumas melhorias na performance do sistema, usando uma implementação de cache comprimido adaptativo no sistema operacional Sprite. Porém, Douglis não conseguiu concluir se o uso de cache comprimido é útil ou não pois seus testes apresentaram resultados divergentes, com melhorias em alguns casos e pioras em outros.\\

Tendo o trabalho de Douglis ser inconclusivo, muitos outros pesquisadores se ocuparam em estudar o caso. Em 1999, Kaplan\cite{kaplan99compressed} chegou à conclusão que a compressão do cache pode reduzir os custos das operações de \textit{paging}\footnote{São operações de E/S no \textit{Page Cache}\cite{love05kerneldevel}. Este cache é utilizado para reduzir o número de E/S dos discos, aumentando assim a performance. Páginas do \textit{Page Cache} estão na memória RAM.}. Kaplan ainda confirmou o que Douglis\cite{douglis93} verificou anteriormente: cache comprimido estático beneficia menos do que um cache comprimido com adaptatividade. Alguns trabalhos correlatos também foram desenvolvidos em 1999. Como apresentado em \cite{swapcc99}, não se trata especificamente da memória cache comprimida, mas sim da compressão da área de \textit{swap} do sistema. Neste trabalho é apresentada uma versão de compressão da memória voltada às páginas que podem ser selecionadas para o \textit{swap}, salvando algum espaço no disco e diminuindo as operações de E/S no \textit{swap}. Testes concluíram que a compressão da área de \textit{swap} pode aumentar a velocidade das aplicações em 20\%. Outros trabalhos também apontaram ganhos na performance de aplicações, como apresentado em \cite{irina05}, no quais a compressão da memória melhorou a performance de aplicações reais, com índices de 1.3 a 55. Nos testes com \textit{benchmarks} e algumas aplicações de simulação, como o NS2 (\textit{Network Simulator}), foi verificado que um nível de memória comprimida adicionada ao sistema garante que aplicações que necessitem de um \textit{working set} maior que a memória física disponível, possam ser executadas.\\

Um dos últimos trabalhos sobre Cache Comprimido foi implementado por Rodrigo Castro \cite{castro03-2, castro03}. Utilizando a versão 2.4.x do Linux \textit{kernel}, vários testes foram realizados, com adaptatividade e sem. Os resultados mostraram que os ganhos são maiores quando existe alguma heurística de adaptatividade presente no CC. Como dito anteriormente, um dos objetivos deste trabalho também é propor uma heurística de adaptatividade para o CC. Porém, pretende-se usar um método mais científico e menos empírico. Através dos SOMs e da classificação de padrões de consumo de memória proposta em \cite{Mlin:06}, espera-se traçar um ``perfil'' de consumo de cada aplicação e usá-lo para definir uma heurística de adaptatividade do CC.\\

A maioria dos trabalhos discutidos anteriormente, são anteriores às versões mais novas do \textit{kernel} do Linux. Daquela época para os dias atuais, o \textit{kernel} sofreu muitas melhorias e o esquema utilizado para o CC na versão usada nesse trabalho é diferente. Assim faz-se necessário um \textit{overview} da versão do Cache Comprimido usada neste trabalho, implementada por Nitin Gupta \cite{briglia07, ccache09}.

\section{Cache Comprimido para Linux \textit{kernel} 2.6.x}

Dados experimentais avaliados em um sistema utilizando Cache Comprimido \cite{briglia07}, mostram que não só as taxas de E/S podem ser melhoradas, como também todo o comportamento do sistema, especialmente em situações de memória crítica, como por exemplo, adiando a chamada do \textit{Out-Of-Memory killer} -- OOM.\\

A implementação atual do Cache Comprimido tira vantagem do sistema de \textit{swap}, adicionando uma área de \textit{swap} virtual como área de armazenamento das páginas comprimidas. Utilizando o algoritmo de compressão usando em sistemas JFFS2 (LZO \cite{obe05lzo, ZivLem77}), páginas alocadas na memória selecionadas para irem para o \textit{swap} são comprimidas e enviadas à uma partição de swap virtual através de um dispositivo virtual de bloco chamado \textit{ramzswap}. Basicamente, o \textit{ramzswap} "engana" o \textit{kernel} do Linux e faz com que seja adicionada uma área de \textit{swap}, que na realidade não é uma partição em um disco, mas sim uma área da memória RAM do dispositivo. O tamanho da memória alocada para essa área é o tamanho do Cache Comprimido.\\

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.7]{figs/compcache.png}
 % compcache.png: 425x122 pixel, 72dpi, 14.99x4.30 cm, bb=0 0 425 122
 \caption{Estrutura de swaps utilizando ramzswap como dispositivo de swap virtual.}
 \label{fig:compcache}
\end{figure}

Uma outra vantagem da versão avaliada do CC é que um \textit{backing-swap device} pode ser configurado, ou seja, pode ser adicionado uma partição real de swap (em disco, por exemplo), a qual as páginas comprimidas podem ir caso falte espaço no \textit{ramzswap}, o dispositivo de bloco virtual criado pelo CC.

\subsection{Design da Implementação}

Utilizando um dispositivo virtual de bloco como partição para \textit{swap}, o Cache Comprimido usado neste trabalho aproveita todo o mecanismo de \textit{swapping} implementado no \textit{kernel} do Linux. Dessa forma, o \textit{Swap Cache} (discutido anteriormente), assim como a seleção de páginas para serem comprimidas, são utilizadas de forma natural pelo algoritmo de Gerenciamento de Memória do \textit{kernel} do Linux.\\

Diferente da versão avaliada em \cite{briglia07}, esta nova versão do Cache Comprimido não é intrusiva e não foi necessário alterar o kernel do Linux. Porém, essa não intrusão custou a compressão de páginas não-anônimas, ou seja, aquelas páginas pertencentes ao Page Cache. Somente páginas que podem ir para uma área de swap são comprimidas na versão atual do Cache Comprimido.\\

Afim de obter maior eficiência e evitar alguns problemas como a fragmentação, foi desenvolvido um escalonador de memória especialmente para o Cache Comprimido. Esse escalonador, chamado xvMalloc\cite{ccache09}, é baseado no escalonador TLSF (\textit{Two Level Segregate Fit})\cite{Masmano04}, desenvolvido para sistemas de tempo real.\\

O xvMalloc foi desenvolvido levando em conta que os alocadores de memória de propósito geral, geralmente são projetados para trabalharem com requisições em número de páginas maiores que 4 \textit{kilobytes}, que é o tamanho padrão para páginas de memória no Linux. Dessa forma, quando utilizados para alocarem porções muito menores, como 32 \textit{bytes} ou $3/4$ do tamanho de uma página, não se obtém uma boa performance e muitas vezes acontece a fragmentação.\\

Herdadas do TLSF\cite{Masmano04}, o xvMalloc\cite{ccache09} possui algumas características que são muito úteis para o Cache Comprimido:

\begin{itemize}
 \item \textbf{Tempo de resposta constante:}\cite{Masmano04} O pior caso de tempo de execução (WCET) do xvMalloc (e do TLSF), para alocar e desalocar uma porção de memória é constante, ou seja, $O(1)$.
 \item \textbf{Eficiência no consumo de memória:}\cite{Masmano04} TLSF vem sido testado em várias situações de consumo de memória, em sistemas operacionais de tempo-real e 
 \item \textbf{Tamanho dos metadados:}\cite{ccache09} em um sistema de 64 bits, o xvMalloc gasta somente 4 bytes por objeto são necessários para localizá-lo e guardar outras informações imprescendíveis.
 \item \textbf{Tamanho do objeto:}\cite{ccache09} Cada objeto (que contém uma página comprimida, ou uma parte dela), possui seu tamanho exato armazenado no cabeçalho, economizando assim mais 2 bytes por objeto. O xvMalloc ainda possui um método chamado \textit{xvGetObjectSize(obj)} que retorna o tamanho do objeto comprimido, esse método é utilizado pelo decompressor.
\end{itemize}

Essas características são importantes para o Cache Comprimido pois são muito utilizadas pelo sistema proposto. Sendo o \textit{ramzswap} nada mais que uma área de memória, é importante que a velocidade de leitura/escrita (nessa caso, de alocação e liberação), seja tão rápida quanto o acesso a uma partição de swap real. A baixa fragmentação da memória também é importante pois no caso do Cache Comprimido, estamos falando de porções de memória que podem ser muito menos que o tamanho padrão de uma página (que é de 4 KB, no Linux). Assim, como as páginas de memória possuem tamanhos bastante variados, é importante utilizar técnicas que garatem uma baixa fragmentação, melhorando assim a velocidade na recuperação de páginas comprimidas ou na liberação de espaços vazios.\\

Na versão anterior do Cache Comprimido apresentada em \cite{briglia07}, o tamanho dos metadados era um problema pois gastava-se muitos bytes para endereçar e guardar as informações das páginas comprimidas. Na versão mais atualizada, usada nesse trabalho, os metadados gastos para endereçar as páginas comprimidas ou para indicar espaços vazios na memória, sofreram alterações e a estrutura dos objetos é discutida na próxima seção.

\subsection{Armazenamento das Páginas Comprimidas}

As páginas comprimidas são armazenadas em estruturas de dados que o xvMalloc gerencia. Essas estruturas são projetadas de modo que se agrupem as páginas de memória que estão livres.\\

O cabeçalho dos objetos que o xvMalloc manipula está representado na figura \ref{fig:obj-header}.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.7]{figs/obj-header.png}
	\caption{Cabeçalho de um objeto manipulado pelo alocador do Cache Comprimido -- xvMalloc.}
	\label{fig:obj-header}
\end{figure}

Onde, cada campo significa:\\

\begin{itemize}
 \item \textit{Size}: tamanho da página comprimida, passado para o xvMalloc pelo compressor.
 \item Prev: offset para a posição do bloco anterior (usado ou livre), relativo ao início do \textit{page frame}.
 \item \textit{Flags}:
	\begin{itemize}
		\item F1: 1 se o bloco estiver usado 0, caso contrário.
		\item F2: 1 se o bloco anterior estiver usado 0, caso contrário.
	\end{itemize}
 \item PAD: Não utilizado se o alinhamento for de 4 bytes.
\end{itemize}

O tamanho da estrutura de dados que representa um cabeçalho de uma página comprimida ou porção dela é de 4 bytes.\\

Para objetos que estão livres, ou seja, podem ser alocados pelo alocador xvMalloc para comportar uma página comprimida, ou uma porção dela, a estrutura de dados que os representa é mostrada na figura \ref{fig:obj-free}.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.7]{figs/obj-free.png}
	\caption{Cabeçalho de um objeto livre do alocador do Cache Comprimido -- xvMalloc.}
	\label{fig:obj-free}
\end{figure}

Na figura \ref{fig:obj-free}, existem campos que apontam para a próximo e o anterior endereços de página. Esses campos são utilizados para acessar rapidamente uma página quando é requisitada pelo kernel do Linux. Cada página é identificada pela tupla \textit{$<$pageNum, offset$>$}.

\subsection{Operações de Inserção e Remoção das Páginas Comprimidas}

\textbf{Inserção de páginas no Cache Comprimido:} a página descomprimida é primeiramente comprimida numa página de \textit{buffer}. O algoritmo de compressão utilizado é o LZO \cite{obe05lzo, ZivLem77}, que já é utilizado em sistemas de arquivo do Linux que utilizam compressão.  Após a compressão, o Cache Comprimido requisita ao ramzswap que armazene a página, sendo $\omega$ o tamanho da página comprimida. Nesse momento, o ramzswap solicita ao alocador xvMalloc que aloque um espaço de memória de tamanho $\omega$. Por fim, o xvMalloc passa ao ramzswap a tupla $<$\textit{pageNum, offset}$>$ como identificador daquela página. Essa tupla substitui o endereço no nodo da \textit{radix tree} que o PFRA gerencia.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.6]{figs/radix_tree}
	\caption{Radix tree padrão, antes da compressão da página.}
	\label{fig:radix-tree}
\end{figure}

\textbf{Remoção de páginas do Cache Comprimido:} do ponto de vista do PFRA e do kernel, a página comprimida tem o mesmo tipo que uma página normal. A única diferença entre a página comprimida e as páginas normais é que, a primeira está armazenada em uma partição de swap chamada ramzswap. Dessa forma, a referência à pagina comprimida, que é passada para a radix tree não se difere da referência das outras páginas, pois o ramzswap é um dispositivo de bloco como os outros. Assim, fica à cargo do Cache Comprimido e do xvMalloc que encontre a página comprimida, descomprime-a e retorne sua referência e valor ao kernel quando o mesmo requisita uma página do ramzswap.

\section{Experimentos com Cache Comprimido}

Nesta seção são apresentados os testes que foram executados utilizando Cache Comprimido em um sistema embarcado com Linux. O principal objetivo dos testes é avaliar os impactos e as características do consumo de memória, quando temos uma área comprimida adicionada ao sistema.\\

Como dito anteriormente, a atual implementação do Cache Comprimido trata dois tipos de páginas: páginas anônimas e páginas do \textit{Page Cache}. O primeiro tipo de páginas foi utilizado como referência pois pode ser feito \textit{swap}, facilitando a obtenção de dados para os gráficos.\\

Foram realizados testes com e sem um \textit{swap} real, utilizando uma partição em um cartão MMC\footnote{Os cartões MMC são utilizados como memória secundária de dispositivos móveis, câmeras fotográficas, etc. Cartões MMC possuem uma taxa de transferência de até 52 MB/seg. A capacidade de armazenamento varia de alguns \textit{megabytes} até cartões com mais espaço de armazenamento, na ordem de \textit{gigabytes}.}. A utilização de um \textit{swap} real se justifica pois é importante comparar o \textit{swap} virtual com o real. Como medidas desta comparação, foram considerados os seguintes itens:

\begin{itemize}
	\item  Quantas páginas são mantidas no Cache Comprimido e não vão para o \textit{swap} real, evitando assim um maior \textit{overhead} por causa das operações de E/S.
	\item Em quais situações o Cache Comprimido é útil para evitar que o OOM (Out-of-memory killer), seja chamado.
	\item Qual é a velocidade para que páginas de memória sejam comprimidas e armazenadas no ramzwap.
\end{itemize}

\subsection{Suite de Testes e Metodologia}
\label{sec:metodologia}

Utilizou-se um dispositivo móvel, parecido com um PDA que possui Linux embarcado como sistema operacional nativo. O Nokia Internet Tablet N810 \cite{nokiaN810} tem um processador ARM1136 com 400Mhz, 128MB de memória RAM e 256MB de memória \textit{flash}, utilizada como armazenamento secundário. Ainda possui acelerador gráfico 2D/3D e dois leitores de cartões MMC/SD.\\

Nota-se que este dispositivo móvel, como muitos outros, foi projetado para aplicações multimídia \cite{nokiaN810}. Este tipo de aplicação demanda muito poder de processamento e quantidade de memória razoável para as aplicações. Levando isso em consideração, os testes foram realizados afim de verificar o comportamento de todo o sistema e das aplicações multimídia, quando a quantidade de memória livre disponível é muito baixa. O objetivo é verificar a performance de todo o sistema, quando se realizam operações de compressão e recuperação das páginas comprimidas.\\

Os casos de uso dos testes podem ser divididos em duas partes: testes que utilizam \textit{benchmarks} sintéticos e testes que simulam a utilização real do aparelho. No primeiro grupo, existe um maior controle do consumo de memória e assim, pode-se avaliar o comportamento do Cache Comprimido quando é submetido à pressões por falta de memória. No segundo grupo, foi verificado se o Cache Comprimido exerce um \textit{overhead} que afeta a performance das aplicações que o usuário pode executar. Espera-se que a performance das aplicações melhore, visto que o número de E/S resultantes de acessos à um dispositivo de \textit{swap} real (dispositivo de bloco), é diminuído pois mais páginas se encontram na memória principal, através do \textit{swap} virtual utilizando o ramzswap..\\

Os testes utilizando aplicações, consistem em:

\begin{itemize}

	\item Executar de 8 a 10 instâncias do navegador WEB, simultaneamente, acessando páginas na Internet através de uma conexão de rede sem-fio.
	\item Tocar um arquivo de vídeo de 7,5MB.
	\item Realizar operações com mídias: tocar mp3, indexar novos arquivos, abrir fotos, etc.
	\item Abrir um documento em formato PDF.
\end{itemize}

Para fazer a interação entre o \textit{X system} e as aplicações, foi utilizado uma ferramenta chamada Xautomation \cite{xauto07}. Com esta ferramenta foi possível controlar toda a movimentação do cursor na tela e os cliques. Os programas usados para realizar os testes com interação automatizada estão presentes no Anexo - I. Enquanto as aplicações são executadas, outros programas ficam responsáveis de fazer a coleta das informações referentes ao consumo de memória, através de leituras das estatísticas exportadas pelo \texttt{procfs}. É com base nesses dados que alguns gráficos foram plotados e discutidos nas seções posteriores deste trabalho.\\

Os testes utilizando \textit{benchmarks} sintéticos, foram executados usando um conjunto de utilitários chamado MemTest \cite{memtest06}. MemTest foi implementado para avaliar a estabilidade e consistência do sistema de gerência de memória do Linux \textit{kernel}. Do MemTest foi utilizado um utilitário chamado \texttt{fillmem}. Este utilitário faz várias alocações de memória, ou seja, de páginas de memória, disparando a necessidade de mandar algumas páginas para o ramzswap, ou seja, o \textit{swap} virtual.

Ambos os tipos de testes, utilizaram cenários pré-definidos, dependendo do que se queria medir. Basicamente, os cenários apresentavam diferentes tamanhos da memória principal, se possuíam uma área de \textit{swap} real ou não, ou se estavam com o Cache Comprimido habilitado ou não.\\

Os testes de comportamento do consumo de memória visam avaliar como a memória é consumida ao longo do tempo de duração do teste. Assim, espera-se identificar os pontos de falta de memória e a atuação do OOM \textit{killer}. Como o Cache Comprimido se utiliza da memória principal para armazenar as páginas comprimidas no \textit{swap} virtual, a quantidade de memória não-comprimida disponível para as aplicações é decrescida, e isso pode levar a uma chamada precoce do OOM killer.

\subsection{Ajustando o kernel}

Antes dos testes serem iniciados, alguns parâmetros do gerenciamento de memória do \textit{kernel} devem ser ajustados, afim de garantir que os testes sejam executados com sucesso.\\

O sistema de \textit{swap} do \textit{kernel} possui alguns parâmetros que podem interferir nos valores medidos, e até na execução das aplicações. Durante os testes, foram alterados dois desses parâmetros:

\begin{itemize}
\item \textbf{swappiness} \cite{love05kerneldevel} é um parâmetro que conFigura um fator de balanço que o \textit{kernel} utiliza para manter mais ou menos páginas no \textit{Page Cache} antes de mandar para a área de \textit{swap}. Seu valor \textit{default} é 60.
\item \textbf{min\_free\_kbytes} \cite{love05kerneldevel} é usado como um limite mínimo de memória livre para que o sistema de gerenciamento de memória virtual do \textit{kernel} comece a mandar páginas para o \textit{swap}.
\end{itemize}

Se o usuário quer que o \textit{kernel} mande mais páginas para o \textit{swap}, o que significa em mais páginas comprimidas, quando o CC está rodando com o \textit{swap} virtual, o parâmetro \texttt{swappiness} deve ser acrescido. Uma outra forma de mandar as páginas mais cedo para o \textit{swap}, é aumentar o valor do \texttt{min\_free\_kbytes}. Durante os testes, foram utilizados valores diferentes desses dois parâmetros em cada cenário, afim de prover diferentes situações de falta de memória.\\


\subsection{Testes de Comportamento do Consumo de Memória}
\label{sec:mem_behavior}

O principal objetivo dos testes realizados nessa seção é classificar e identificar padrões de consumo de memória ao longo do tempo para determinadas aplicações.\\

As aplicações escolhidas para os testes foram: um navegador de Internet ou \textit{browser}, um \textit{media player} chamado Canola e um programa para ler arquivos no formato PDF. O ambiente de testes é um dispositivo com Linux embarcado e 128MB de memória RAM total. Para realizar as coletas em diferentes situações e ter um comparativo, foram ainda definidos três cenários:

\begin{itemize}
 \item Cenário Alpha: Cache Comprimido desligado e kernel sem área de swap.
 \item Cenário Beta: Cache Comprimido ligado e configurado com área de swap virtual aproximadamente igual a 10 MB.
 \item Cenário Gama: Cache Comprimido ligado, configurado com área de swap virtual aproximadamente igual a 10 MB e com \textit{swap balance} igual a 60.
\end{itemize}

Os cenários servem para identificar em quais situações é bom ter um CC configurado ou não, ou ainda qual o impacto de inserir uma área de swap na memória e comprimir as páginas. Também servem para dar um panorama e ajudar na definição de uma heurística boa para implementar a adaptatividade do CC, um dos objetivos desse trabalho.\\

Para padronizar os testes e simular a mesma situação nos três cenários dados, foram desenvolvidos programas para automatizar a interação com as aplicações utilizadas nos testes. O XAutomation \cite{xauto07}, é um utilitário para Linux que serve para interagir com o sistema gráfico do sistema, fazendo o papel dos cliques de mouse, por exemplo. Assim é possível automatizar a abertura de páginas da Internet nos testes envolvendo o \textit{browser}, automatizar também a interação com o Canola e o visualizador de PDF's.

\subsubsection{Resultados dos testes}

Os experimentos, como dito anteriormente, foram divididos em três cenários: Alpha, Beta e Gama. Os resultados para cada teste com as aplicações estão representados nos gráficos abaixo:\\

\textbf{Testes com o browser}\\

\textit{Cenário Alpha versus cenário Beta}\\

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-behavior-browser-alpha-beta.pdf}
 \caption{Browser: memória livre (MemFree) ao longo do teste. A linha pontilhada representa o consumo quando o CC estava configurado para 10MB de tamanho.}
 \label{fig:mem-behavior-browser-alpha-beta}
\end{figure}

\textit{Cenário Alpha versus cenário Gama}\\

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-behavior-browser-alpha-gama.pdf}
 \caption{Browser: A linha pontilhada representa o consumo quando o CC estava configurado para 10MB de tamanho e swappiness = 60.}
 \label{fig:mem-behavior-browser-alpha-gama}
\end{figure}

A memória livre no segundo teste é maior pois mais páginas de memória são enviadas mais cedo para a área de swap, nesse caso, o \textit{ramzswap} do Cache Comprimido. Isso acontece pois o \textit{kernel} foi configurado com \textit{swappiness} $= 60$, e no gráfico anterior, esse valor era $0$. A Tabela \ref{tab:browser-compcache} mostra estatísticas lidas do Cache Comprimido, comparando o cenário com \textit{swappiness} $= 0$ e o outro com \textit{swappiness} $= 60$.\\

\begin{table}[h]
\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
 swappiness & Num. páginas compr. & Tam. original (KB) & Tam. comprimido (KB) & Mem. usada (KB) \\ \hline
 = 0 & 632 & 2528 & 857 & 880 \\ \hline
 = 60 & 2544 & 10176 & 3337 & 3520 \\ \hline
\end{tabular}
\caption{Dados do Cache Comprimido dos cenários Beta e Gama para o Browser.}
\label{tab:browser-compcache}
\end{table}

A Tabela \ref{tab:browser-compcache}, mostra que quando o swappiness é maior, a quantidade de páginas armazenadas e comprimidas pelo CC também é maior. Assim, em situações de grande consumo de memória é bom ter um swappiness com um valor maior que zero pois mais páginas são comprimidas. Nos testes podemos ver que o tamanho original da memória ocupada pelas páginas, quando o swappiness é igual a 60, é de 10176 KB, após a compressão, somente 3337 KB é armazenado na área de swap virtual do CC, o ramzswap. Essa dado mostra que no teste foi possível armazenar as páginas de memória gastando-se $1/3$ da memória sem compressão. Mesmo quando o swappiness é zero, a taxa de compressão continua muito boa pois temos 2528 KB de páginas sem compressão e apenas 857 KB de memória após a compressão.\\


\textbf{Testes com o Canola}\\

\textit{Cenário Alpha versus cenário Beta}\\

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-behavior-canola-alpha-beta.pdf}
 \caption{Canola: memória livre (MemFree) ao longo do teste. A linha pontilhada representa o consumo quando o CC estava configurado para 10MB de tamanho.}
 \label{fig:mem-behavior-canola-alpha-beta}
\end{figure}

\textit{Cenário Alpha versus cenário Gama}

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-behavior-canola-alpha-gama.pdf}
 \caption{Canola: memória livre com Cache Comprimido = 10MB e swappiness = 60.}
 \label{fig:mem-behavior-canola-alpha-gama}
\end{figure}

Nos gráficos do Canola podemos notar que a memória livre se manteve estável para os 2 cenários testados. Também podemos concluir que isso aconteceu porque o Canola não aloca muitas páginas que podem ir para o swap ou ainda, não alocou uma grande quantidade de memória.\\

Analisando os dados do CC para os cenários Beta e Gama, temos:

\begin{table}[h]
\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
 swappiness & Num. páginas compr. & Tam. original (KB) & Tam. comprimido (KB) & Mem. usada (KB) \\ \hline
 = 0 & 15 & 60 & 1 & 4 \\ \hline
 = 60 & 33 & 132 & 1 & 4 \\ \hline
\end{tabular}
\caption{Dados do Cache Comprimido dos cenários Beta e Gama para o Canola.}
\label{tab:canola-compcache}
\end{table}

Na Tabela \ref{tab:canola-compcache}, os dados indicam que a área de memória comprimida do CC não foi muito utilizada, nem quando o \textit{kernel} foi configurado com swappiness igual a 60.\\

\textbf{Testes com o PDF viewer}\\

Nos dados coletados para o leitor de PDF's, os gráficos de consumo de memória se apresentaram de forma idêntica para os 3 cenários (veja Figura \ref{fig:mem-behavior-pdf-alpha-beta-gama}). Isso se deve ao fato da aplicação testada não ter atingido um consumo alto de memória, acionando assim o uso da área de swap.\\

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-behavior-pdf-alpha-beta-gama.pdf}
 \caption{PDF: comportamento de consumo da memória para os 3 cenários.}
 \label{fig:mem-behavior-pdf-alpha-beta-gama}
\end{figure}

\subsubsection{Comparando os resultados}

Apresentados todos os resultados para cada aplicação, podemos tirar algumas conclusões:

\begin{itemize}
 \item O browser é a aplicação que mais consumiu a memória nos testes.
 \item O Canola teve um consumo mediano. O uso do CC foi baixo, com poucas páginas comprimidas.
 \item Foi confirmado que o parâmetro de \textit{swappiness} influencia o consumo de memória das aplicações.
 \item Não foi verificada qualquer diferença no consumo da aplicação de leitura de PDF's.
\end{itemize}

Com esses dados, podemos ainda concluir que o CC traz mais benefícios para o sistema em situações de alto consumo de memória e com swappiness igual a 60, fazendo com que mais páginas de memória sejam comprimidas. Para aplicações que possuem consumo de memória mediano, o CC pode ter um tamanho menor pois não é tão utilizado. Para aplicações com consumo de memória muito baixo, o CC não traz nenhum benefício.


\subsection{Testes com o Mibench}
\label{sec:mibench}

MiBench \cite{GRE01} é um conjunto de \textit{benchmarks} sintéticos que simulam vários tipos de operações afim de testar a performance de uma arquitetura ou de um sistema embarcado.\\

A principal diferença do MiBench para os outros \textit{benchmarks} para sistemas embarcados é que o primeiro é composto por programas que possuem o código aberto, ou seja, são \textit{Open Source}. Uma outra diferença é que o MiBench é dividido em vários pacotes \cite{GRE01}, ou módulos, divididos de acordo com o tipo de benchmark que deseja-se realizar, como mostrado a seguir:

\begin{itemize}
 \item \textit{Automotive and Industrial Control}: possui programas que visam a avaliação de microprocessadores dedicados ao controle de sistemas. Os programas usados nessa categoria avaliam as operações matemáticas, contagem de bits, ordenação e processamento de imagens.
 \item \textit{Network}: possui programas destinados a testarem microprocessadores existentes em roteadores e \textit{switches}. Os testes encontrados nesse módulo visam avaliar algoritmos e operações referentes à cálculos para encontrar rota mais curta, busca em árvores e tabelas de entrada/saída de dados.
 \item \textit{Security}: este módulo inclui programas que avaliam os algoritmos de encriptação, decriptação e \textit{hash} de um microprocessador. É mais voltado para operações matemáticas.
 \item \textit{Consumer Devices}: esta categoria é a mais parecida com o propósito deste trabalho. Possui programas que avaliam o processamento de imagem, áudio e HTML. Os programas incluídos nesta categoria fazem uso do processador e da memória para tocar uma música ou aplicar transformações em uma fotografia, por exemplo.
 \item \textit{Office Automation}: esta categoria, utiliza programas que avaliam a manipulação de texto afim de realizar testes de performance em equipamentos de escritório, tais como impressoras, aparelhos de fax e processadores da fala.
 \item \textit{Telecommunications}: neste módulos estão presentes programas que avaliam a capacidade dos microprocessadores dedicados à comunicação, codificação e decodificação de voz.
\end{itemize}

Os testes realizados neste trabalho visam avaliar se o uso do Cache Comprimido é importante ou prejudicial na execução de alguns \textit{benchmarks} presentes no MiBench. Para a realização dos testes, foi escolhido um programa de cada categoria apresentada anteriormente.\\

A seguir encontram-se os gráficos do consumo da memória e do consumo da área de swap para cada categoria do MiBench. Foi escolhido um \textit{benchmark} de cada categoria:

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/automotive.png}
 % automotive.png: 640x480 pixel, 72dpi, 22.58x16.93 cm, bb=0 0 640 480
 \caption{Teste com o benchmark da categoria Automotive}
 \label{fig:mibench_automotive}
\end{figure}

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/consumer.png}
 \caption{Teste com o benchmark da categoria Consumer}
 \label{fig:mibench_consumer}
\end{figure}

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/network.png}
 \caption{Teste com o benchmark da categoria Network}
 \label{fig:mibench_network}
\end{figure}

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/office.png}
 \caption{Teste com o benchmark da categoria Office Automation}
 \label{fig:mibench_office}
\end{figure}

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/security.png}
 \caption{Teste com o benchmark da categoria Security}
 \label{fig:mibench_security}
\end{figure}

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/telecom.png}
 \caption{Teste com o benchmark da categoria Telecom}
 \label{fig:mibench_telecom}
\end{figure}

Como pode-se notar nos gráficos de consumo de memória dos testes com o MiBench (Figuras \ref{fig:mibench_automotive}, \ref{fig:mibench_consumer}, \ref{fig:mibench_network}, \ref{fig:mibench_office}, \ref{fig:mibench_security}, \ref{fig:mibench_telecom}), a utilização do Cache Comprimido nesse caso não traz nenhum benefício. A diferença da memória livre inicial vista nos gráficos, deve-se ao fato de que, quando o sistema possui área do Cache Comprimido configurada, a memória total livre é menor do que se o Cache Comprimido estivesse desligado. Isso deve-se ao fato do ramzswap alocar uma área da memória principal para armazenar as páginas comprimidas.\\

Os gráficos indicam que os benchmarks do MiBench não são ideais para avaliarem consumo de memória quando não é utilizado um simulador de arquitetura. Por utilizarem muito mais o processador para operações, os benchmarks são projetados para avaliarem número de instruções efetuadas em várias regiões do microprocessador e não o consumo de memória do dispositivo.

\subsection{Testes de Performance}

Nesse tipo de teste foi utilizado um \textit{benchmark} sintético que realize a alocação de uma grande área de memória. O objetivo é analisar o overhead imposto pelo Cache Comprimido quando as páginas são destinadas à área de \textit{swap} virtual (ramzswap).\\

Para testar a alocação de memória, foi utilizado um programa da suíte de testes MemTest\cite{memtest06}. O programa em questão é o \textbf{fillmem} e ele foi usado para alocar uma grande quantidade de memória de forma muito rápida. Na figura \ref{fig:fillmem50} estão os tempos de execução do fillmem para alocar 50 MB de memória.\\

\begin{figure}[htpb]
 \centering
 \includegraphics[scale=0.5]{figs/fillmem50.png}
 % fillmem50.png: 640x480 pixel, 72dpi, 22.58x16.93 cm, bb=0 0 640 480
 \caption{Tempo de de execução do fillmem para alocar 50 MB.}
 \label{fig:fillmem50}
\end{figure}

No gráfico mostrado na figura \ref{fig:fillmem50}, nota-se que o tempo de alocação do fillmem é cerca de três vezes maior quando se usa o Cache Comprimido. Isso deve-se ao fato de haver overheads para a compactação das páginas. A máquina usada \cite{nokiaN810}, como visto anteriomente, não possui disco rígido e sua memória secundária é composta por memória do tipo flash e cartões MMC. Esses dois tipos de memória, por suas características, possui um tempo de acesso muito maior do que o tempo dos discos rígidos convencionais. Por essa razão, a diferença entre ter uma memória de \textit{swap} virtual, e ter um swap real, tanto na memória \textit{flash}, quanto no MMC, não garante que o acesso ao \textit{swap} virtual do Cache Comprimido será mais rápido. Como pode-se notar no gráfico da figura \ref{fig:fillmem50}, o acesso às páginas de memória armazenadas no \textit{swap} real, em um cartão MMC, é até um pouco maior do que o tempo de acesso no Cache Comprimido. Isso deve-se ao fato de que o acesso ao \textit{swap} no MMC não possuir os tempos de descompactação da página.

\section{Sumário}

Neste Capítulo foram apresentados os detalhes de implementação e decisões de projeto do Cache Comprimido utilizado neste trabalho. Também foram apresentadas as diferenças entre essa versão e as versões utilizadas nos trabalhos relacionados ao Cache Comprimido. Ainda foram apresentados os resultados dos testes com o Cache Comprimido quando executados em Linux embarcado e com cenários de uso pré-definidos, que representassem o uso normal do Linux embarcado. Esses cenários serão utilizados no Capítulo 4, no treinamento da rede neural e fabricação do SOM.\\

Um dado constatado no teste de consumo de memória é que o Cache Comprimido é mais utilizado quando o kernel é pré-configurado para mandar as páginas de memória mais cedo para o swap. Para o kernel, é transparente o uso do swap virtual do Cache Comprimido, desta forma não se interfere no algoritmo utilizado para selecionar as páginas e enviá-las. Configurando o swapness com o valor padrão de 60, as páginas de memória são enviadas para o Cache Comprimido antes do sistema atingir uma situação crítica de falta de memória.\\

Um outro dado, não menos importante, é que o overhead de comprimir e descomprimir as páginas e localizá-las dentro do swap virtual é bastante próximo do overhead padrão de utilização de um swap real, ou seja, de um swap utilizando uma partição em dispositivo de bloco (como um MMC card). Verificando o gráfico apresentado na seção de Testes de Performance, pode-se notar a diferença mínima entre o tempo de execução do teste em cada cenário.



\chapter{Redes Neurais e Mapas Auto-organizáveis no Cache Comprimido}

Este capítulo apresenta uma introdução sobre redes neurais e mapas auto-organizáveis. Também apresenta como funciona o método de classificação de consumo de memória proposto em \cite{Mlin:06}, e utilizado neste trabalho nos testes com o Cache Comprimido.

\section{Redes Neurais Artificiais}
\label{sec:redes_artificiais}

As \textit{redes neurais artificiais} podem ser caracterizadas como modelos computacionais, com certas propriedades particulares como a habilidade de aprender, de identificar e classificar padrões \cite{Ben93}. Desta maneira, uma rede neural é capaz de extrair regras básicas a partir de dados reais, ou seja, a sistemática do problema, diferindo assim da computação programada, onde é necessário um conjunto de regras rígidas pré-fixadas e algoritmos \cite{Haykin:99}.\\

Uma rede neural artificial consiste em \cite{Ben93}:

\begin{itemize}
 \item Um conjunto de unidades de processamento.
 \item Um estado de ativação para cada uma dessas unidades, que equivale aos dados de saída.
 \item Conexões entre as unidades (ou neurônios). Essas conexões possuem pesos que interferem nos sinais de saída das unidades vizinhas.
 \item Uma regra de propagação que ajusta a entrada de dados nas unidades de processamento. Esses dados são provenientes de uma amostra externa.
 \item Uma função de ativação, que determina o novo nível de ativação baseada na entrada efetiva e no nível corrente de ativação.
 \item Um conjunto de dados externos ou amostra para a entrada de dados de cada unidade de processamento.
 \item Uma regra de aprendizagem.
 \item Um ambiente onde a rede neural deve ser inserida. Esse ambiente deve prover os dados de entrada, sinais e códigos de erros se necessário. Geralmente esse ambiente é o sistema operacional ou um sistema de simulação.
\end{itemize}


Existem duas abordagens de treinamento para as redes neurais: o supervisionado e o não-supervisionado \cite{Haykin:99}.

\begin{itemize}
 \item Treinamento supervisionado: nesse tipo de treinamento é fornecido um par de entrada e saída desejada de acordo com os dados da amostra. É calculado também um erro que é utilizado como retro-alimentação da rede afim de ajustar os pesos sinápticos dos neurônios.
 \item Treinamento não-supervisionado: nesse tipo de treinamento somente são fornecidos os dados de entrada. O próprio mecanismo da rede deve ser capaz de extrair informações estatísticas das amostras afim de ajustar os pesos sinápticos os neurônios da rede.
\end{itemize}

Independente do tipo de aprendizagem ou treinamento selecionado, toda rede é alimentada por uma seqüência de valores $x_{1}, x_{2}, \cdots, x_{n}$ na entrada e os pesos são ajustados de acordo com um modelo matemático durante a fase de treinamento. O processo de treinamento pode ser organizado nas etapas seguintes \cite{Haykin:99, Ben93}:

\begin{enumerate}
	\item O primeiro padrão de entrada é apresentado para a rede.
	\item Os pesos são ajustados para capacitar a rede e reconhecer o padrão fornecido.
	\item O segundo padrão de entrada é apresentado para a rede e a etapa 2 é efetuada novamente.
	\item O mesmo é aplicado para todos os outros padrões.
	\item O procedimento de 1 até 4 é executado novamente centenas ou milhares de vezes até encontrar uma configuração de pesos sinápticos capaz de reconhecer todos os padrões fornecidos no treinamento.
\end{enumerate}

Este trabalho utiliza-se de uma rede neural com treinamento não-supervisionado para encontrar padrões de consumo de memória. No trabalho encontrado em \cite{Mlin:06}, pode-se verificar que é possível identificar padrões referentes à quantidade de páginas de memória que são requisitadas ao \textit{kernel} do Linux.\\

Como cada página de memória é atômica, ou seja, está ou não está completamente ocupada, é possível gerar amostras de entrada para a rede neural com base na velocidade que as páginas são requisitadas ou ocupadas. Cada programa requisita memória de acordo com o seu fluxo de execução e é com base nessa caracaterística que foi comprovado que é possível classificar o consumo de memória \cite{Mlin:06}.\\

Após encontrados os perfis de consumo de memória das aplicações testadas nos casos de uso, espera-se adaptar o uso do Cache Comprimido de acordo com o comportamento das mesmas. Para aplicações que requisitam mais memória, o Cache Comprimido deve ser um pouco maior, esperando aumentar a capacidade de armazenamento das páginas de memória. Para aplicações que possuem um consumo menor, o Cache Comprimido pode até ser descartado. Porém, não podemos esquecer de uma importante variável: a velocidade de consumo da memória.\\

Como visto na seção de testes inciais deste trabalho (ver detalhes capítulo 2), aplicações que requisitam memória muito rapidamente, podem levar o sistema à falta de memória, mesmo que o Cache Comprimido esteja ativo. Para evitar isso, é necessário que se identifique esse perfil antes mesmo que a memória seja requisitada, de forma adequada para otimizar o consumo de memória de uma aplicação.

\section{Mapas Auto-Organizáveis}
\label{sec:som}

\textit{Mapas Auto-Organizáveis} ou simplesmente \textit{SOM (do inglês Self Organizing Maps)} é uma rede neural artificial auto-organizável, de aprendizagem não supervisionada, baseada em grades de neurônios artificiais onde os pesos são adaptados em conformidade com os vetores de entrada fornecidos durante o treinamento. Foi desenvolvido pelo professor Teuvo Kohonen e às vezes é chamado de mapa de Kohonen \cite{Junkie, Borgelt:00, Germano:99, Honkela:98, Koh88}.\\

Os SOMs são um tipo de rede neural um pouco diferente pois utilizam a distância topológica entre os neurônios como parâmetro de entrada do sistema de aprendizagem. Os SOMs são muito utilizados para fazer mapas em que os padrões podem ser visualizados, ou seja, características semelhantes dos dados de entrada (ou amostra), são agrupados topologicamente próximos. Portanto um SOM é caracterizado pela formação de um mapa topográfico dos padrões de entrada, baseado nas características estatísticas intrínsecas contidas nesses dados, por isso o nome \textit{mapa auto-organizável} \cite{Haykin:99}.\\

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.30\textwidth]{figs/training}
	\caption{Exemplo de rede n\~{a}o treinada.\cite{Mlin:06}}
	\label{fig:training}
\end{figure}

Em \cite{Mlin:06} e \cite{Alecrim:07} foi usado um exemplo comum de SOM que utiliza um mapeamento de cores a partir de seus 3 componentes: vermelho, verde e azul ou RGB (do inglês \textit{red, green and blue}) em um espaço bidimensional. Na Figura \ref{fig:training}, pode-se ver um exemplo de SOM que ainda não foi treinado. Já a Figura \ref{fig:som_demo} ilustra um exemplo de SOM treinado para reconhecer padrões de cores RGB. As cores são fornecidas para a rede como vetores de 3 dimensões, uma dimensão para cada componente de cor, e a rede foi treinada para representá-los em um espaço de 2 dimensões. Observe que além do agrupamento de cores em regiões distintas, as regiões de propriedades similares estão localizadas de forma adjacente \cite{Mlin:06}.\\

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.30\textwidth]{figs/som_demo}
	\caption{Um exemplo de SOM treinado para classificação de cores.\cite{Mlin:06}}
	\label{fig:som_demo}
\end{figure}

A rede SOM ilustrada na Figura \ref{fig:som_demo} apresenta uma grade de tamanho $40 \times 40$. Cada nó da grade possui 3 pesos, cada um representando um componente RGB. Além disso, cada nó é representado por uma célula retangular, quando desenhado na interface gráfica do aplicativo utilizado para treinar a rede e mostar o SOM.\\

A formação do SOM \'{e} feita primeiramente inicializando os pesos sinápticos da grade, com valores obtidos de um gerador de números randômicos. Assim, nenhuma organização prévia é imposta ao mapa de características (veja Figura \ref{fig:training}. Depois que a grade \'{e} feita, s\~{a}o executados 6 processos importantes \cite{Haykin:99}:

\begin{description}
	\item[Inicialização:] Cada neurônio tem os seus pesos sinápticos inicializados aleatoriamente. Geralmente os pesos são inicializados entre 0 e 1, ou seja, $0 < w < 1$.
	\item[Seleção de Vetor de Entrada:] Um vetor de entrada é escolhido do conjunto de dados de treinamento e apresentado para a grade de neurônios.
	\item[Competição de Neurônios:] Todos os pesos de todos os neurônios são calculados para determinar o neurônio mais semelhante em relação ao vetor de entrada. O neurônio mais semelhante \'{e} selecionado e é considerado como o neurônio vencedor é chamado de Unidade de Melhor Casamento ou BMU (do \textit{inglês Best Matching Unit}).
	\item[Cooperação de Neurônios:] O \textit{raio da vizinhança topológica} do BMU é calculado. O valor do raio assume inicialmente um valor elevado, geralmente tem o mesmo raio da grade, mas diminui a cada iteração do treinamento. Os neurônios localizados dentro deste raio são considerados os vizinhos do BMU.
	\item[Adaptação Sináptica:] Os pesos sinápticos de cada neurônio vizinho do BMU são ajustados para torná-los similares ao vetor de entrada. Os neurônios vizinhos mais próximos do BMU têm os seus pesos alterados de modo mais significativo.
	\item[Repetição:] O segundo passo é retomado novamente selecionando um novo vetor de entrada e os passos subseqüentes são então executados.
\end{description}

A forma para determinar o BMU é acessar todos os neurônios da grade e calcular a \textit{distância Euclidiana} entre o vetor peso de cada neurônio e o vetor de entrada atual. O neurônio de vetor peso mais próximo do vetor de entrada (menor distância Euclidiana), é considerado como o neurônio vencedor ou BMU. Neste caso a distância Euclidiana é a função discriminante neste processo competitivo de neurônios e calculado como ilustra a Equação \ref{eq:euclidean}.

\begin{equation}
dist_{j} = \sqrt{\sum_{i=1}^n (x_{i}-w_{i})^2}
\label{eq:euclidean}
\end{equation}

Os pesos sinápticos dos neurônios são ajustados de acordo com o BMU a cada iteração do treinamento. No decorrer do treinamento, o raio de ação do BMU diminui, modificando menos neurônios vizinhos e especializando ainda mais as áreas com relação aos padrões de entrada da amostra.\\

Os algoritmos usados para a inicialização dos pesos sinápticos, cálculo para achar o BMU e o cálculo da distância euclidiana são os mesmos que foram usados em \cite{Mlin:06} e estão presentes no Apêndice A.

\section{Método de classificação de consumo de memória baseado em SOMs}
\label{sec:classe_rede}

Ainda baseado em \cite{Mlin:06}, essa seç\~{a}o mostra como os SOMs podem ser utilizados para classificar o consumo de memória e também como é feita a coleta dos dados de entrada para o treinamento das redes neurais e conseguinte geração dos SOMs.\\

As coletas consistem em executar casos de uso de aplicações (Browser, Canola e PDF viewer), afim de realizar algumas medições do consumo de memória das mesmas. Os casos de uso foram executados no sistema operacional Linux e seguindo a metodologia apresentada na seção \ref{sec:metodologia}.\\

Como utilizado nos trabalhos \cite{Mlin:06} e \cite{Alecrim:07}, o algoritmo de treinamento da rede espera dados referentes ao consumo de memória de uma determinada aplicação. Ou seja, o programa de treinamento da rede neural espera um vetor com $x$ entradas, sendo $x$ o número de leituras da memória consumida ao longo do tempo.\\

Para cada aplicação testada, foram feitas leituras durante um tempo $t$. Essas leituras formaram o vetor de entrada para treinamento da rede neural e geração do SOM. Assim, o SOM gerado indica o padrão de consumo de memória de todas as aplicações juntas. Cada aplicação ainda tem o seu próprio padrão, agora identificável pela rede neural e SOM gerado. Esse padrão é classificado de acordo com a metodologia desenvolvida em \cite{Mlin:06}.\\

Lembrando que \cite{Mlin:06}:

\begin{itemize}
\item \textit{memória} (mem) é a quantidade de páginas físicas alocadas que representa a \textit{quantidade de memória física consumida};

\item \textit{variação do consumo de memória (VCM)} que é utilizada para indicar o ritmo em que o consumo de memória está aumentando ou diminuindo. Esta variação é calculada usando a razão entre a diferença da quantidade de memória física consumida e o intervalo de tempo decorrido, conforme a Equação \ref{eq:velocidade};

\begin{equation}
VCM = \frac{mem_{2} - mem_{1}}{t_{2} - t_{1}}
\label{eq:velocidade}
\end{equation}

\item \textit{taxa de variação do consumo de memória (TVCM)} que é utilizada para indicar o ritmo em que a variação de consumo de memória está aumentando ou diminuindo. Esta taxa é calculada usando a razão entre a diferença da variação do consumo de memória e o intervalo de tempo decorrido, conforme a Equação \ref{eq:aceleracao}.
\end{itemize}

\begin{equation}
TVCM = \frac{vcm_{2} - vcm_{1}}{t_{2} - t_{1}}
\label{eq:aceleracao}
\end{equation}

Ainda como visto em \cite{Mlin:06}, essas três dimensões ainda possuem subintervalos que podem ser caracterizados como Low (L), Medium (M) e High(H). Esses subintervalos são utilizados para representarem o comportamento de uma determinada propriedade de uma forma simples e abstrata.\\

A quantidade de memória consumida está localizada em um desses subintervalos, assumindo um dos três estados $\{Low, Medium, High\}$ apresentados. Sendo assim a quantidade de memória consumida é classificada como \textit{baixo consumo} de memória quando o seu valor está no estado L, \textit{médio consumo} de memória quando o seu valor está no estado M e \textit{alto consumo} de memória quando localizado no estado H. A quantidade de memória consumida abrange somente valores inteiros positivos, visto que consumo de memória de valor negativo é logicamente inexistente no mundo real. A mesma analogia \'{e} utilizada para os valores de VCM e TVCM.\\

Assim, a tripla $<$memória,vcm,tvcm$>$ pode ser mapeada facilmente para uma estrutura de dados que seja aceita no algoritmo do SOM, que foi usado em \cite{Mlin:06}. Já que o SOM utiliza vetores tridimensionais como valores de entrada, a tripla pode facilmente ser utilizada, contendo cada valor como sendo uma dimens\~{a}o desse vetor.\\

Cada vetor no espaço tridimensional é uma instância específica da tripla que é mapeado na grade de neurônios do SOM. Cada célula da grade de neurônios armazena então um vetor tridimensional do tipo $<$memória, vcm, tvcm$>$, conforme a Figura \ref{fig:mapeia}.\\

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.55\textwidth]{figs/mapeia}
	\caption{Os vetores são mapeados em um espaço bidimensional representado pela grade de neurônios \cite{Mlin:06}.}
	\label{fig:mapeia}
\end{figure}

A idéia de utilizar o mapa de neurônios auto-organizáveis é agrupar topologicamente as classes apresentadas de consumo de memória em áreas ou regiões distintas. Dessa forma, cada região representa uma configuração ou conjunto de configurações similares capaz de representar o estado do consumo de memória de uma aplicação em um determinado instante.

\section{Coleta dos dados}
Como dito anteriormente, os dados coletados para a entrada da rede neural foram obtidos de leituras constantes da memória livre do sistema. Alguns casos de uso foram definidos com base nos mesmos casos apresentados na seção \ref{sec:mem_behavior}. Lá, foram utilizadas as seguintes aplicações: browser, media player e visualizador de arquivos no formato PDF.\\

Todas as leituras foram realizadas no dispositivo móvel e foram desenvolvidos programas que automatizassem a coleta dos dados. Os passos para gerar as amostras foram os seguintes:

\begin{enumerate}
 \item Executa-se um caso de uso, ou seja, somente o browser, ou o media player, etc.
 \item Durante a execução são coletadas as amostras de memória total livre $\times$ tempo.
 \item As amostras são convertidas em triplas para o tipo usado no treinamento da rede neural: $<$memória, vcm, tvcm$>$.
\end{enumerate}

No final, temos 3 arquivos de log contendo a memória total livre durante a execução das 3 aplicações: browser, media player e visualizador de PDF's. Ainda foi adicionado mais um arquivo de log contendo a memória livre $\times$ tempo durante a execução dos 3 programas simultaneamente.\\

Esses arquivos de log são concatenados e utilizados como dados de entrada para o treinamento da rede neural.

\section{Treinando a rede neural e criando o SOM}
\label{sec:treinando-redes}
Foram treinadas diferentes redes neurais com seus SOMs respectivos. A idéia é treinar uma rede neural e gerar o SOM para cada cenário usado nos testes de comportamento de memória (ver seção \ref{sec:mem_behavior}):

\begin{itemize}
 \item Cenário Alpha: Cache Comprimido desligado e kernel sem área de swap.
 \item Cenário Beta: Cache Comprimido ligado e configurado com área de swap virtual aproximadamente igual a 10 MB.
 \item Cenário Gama: Cache Comprimido ligado, configurado com área de swap virtual aproximadamente igual a 10 MB e com \textit{swappiness} igual a 60.
\end{itemize}

Para cada cenário apresentado foram realizadas medidas da memória consumida pelas aplicações durante a execução dos testes. Os testes seguem a mesma metodologia apresentada na seção \ref{sec:mem_behavior} e essas amostras fizeram parte do vetor de entrada para treinamento das redes e geração de cada SOM. No final do treinamento das redes, temos um SOM para o consumo de memória das aplicações quando são executadas no cenário Alpha, um SOM que representa o consumo de memória das aplicações para o cenário Beta e por último um SOM que representa o consumo da memória para o cenário Gama. Essas 3 redes neurais e seus respectivos SOMs são capazes de identificar o consumo de cada uma das aplicações utilizadas nos testes (Browser, Canola ou PDF viewer), de acordo com o cenário usado.\\

Nas Figuras \ref{fig:som-trained}, \ref{fig:som-trained-cc}, \ref{fig:som-trained-cc-tun}, podemos visualizar que os mapas são levemente diferentes em cada um dos três cenários apresentados. Essa diferença nos SOMs indica que o uso do Cache Comprimido altera as classes de consumo de memória das aplicações. De acordo com \cite{Mlin:06}, uma área mais avermelhada, ou seja, com mais elemento \textit{Red} indica que a quantidade de memória medida é grande (mem) e uma área mais azulada (\textit{Blue}), indica que taxa de variação do consumo (tvcm) é alta e o consumo é baixo. Porém, o mais importante é a área de visitação que cada aplicação possui. É esta área que será utilizada para identificar qual aplicação está sendo executada no momento e com isso gerar uma tomada de decisão para o Cache Comprimido (ver seção \ref{sec:classificando}).

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.40\textwidth]{figs/trained}
	\caption{Cenário Alpha: SOM para rede neural treinada sem Cache Comprimido.}
	\label{fig:som-trained}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.40\textwidth]{figs/trained-cc}
	\caption{Cenário Beta: SOM para rede neural treinada com Cache Comprimido de tamanho igual a 10MB.}
	\label{fig:som-trained-cc}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.40\textwidth]{figs/trained-cc-tun}
	\caption{Cenário Gama: SOM para rede neural treinada com Cache Comprimido = 10MB e swappiness = 60.}
	\label{fig:som-trained-cc-tun}
\end{figure}

Nos testes com o cenário Alpha, nenhuma página de memória era comprimida e podemos notar que a parte azulada do gráfico, que indica uma baixa aceleração no consumo e um baixo consumo de páginas de memória, é bem pequena. Nos cenários seguintes, Beta e Gama, a área azulada já é ligeramente maior. Isso se deve ao fato do Cache Comprimido armazenar mais páginas de memória, deixando assim páginas livres na área não-comprimida, indicando um consumo menor da memória, por parte das aplicações.\\

Também podemos notar que de acordo com que o Cache Comprimido é mais utilizado, ou seja, mais páginas de memória são comprimidas e armazenadas, temos um aumento da área azulada no SOM. Isso pode ser verificado no cenário Gama, o qual, através da configuração do parâmetro de \textit{swappiness} em \texttt{/proc/sys/vm/swappiness}, apressamos o envio de páginas de memória para a área de \textit{swap virtual}, deixando assim mais memória livre.

\section{Classificando as aplicações}
\label{sec:classificando}
As triplas $<$memória, vcm, tvcm$>$, são mapeadas para valores RGB das cores de cada pixel que compõe o SOM, sendo que:

\begin{itemize}
 \item a memória é mapeada para R ou vermelho.
 \item o vcm é mapeado para G ou verde.
 \item o tvcm é mapeado para B ou azul.
\end{itemize}

Como mostrado na seção \ref{sec:treinando-redes}, a rede neural que gerou cada SOM é treinada com coletas da memória livre feitas durante a execução de cada aplicação testada. Assim, o SOM agrupa os BMU's (Best Matching Units), de acordo com a classe de consumo, ou seja, de acordo com os valores das triplas $<$memória, vcm, tvcm$>$, mapeadas como cores dos pixels.\\

O SOM representa o padrão de consumo de todas as aplicações testadas juntas (Browser, Canola e PDF viewer). Se quisermos saber qual é o SOM de uma determinada aplicação (somente do Browser, por exemplo), devemos passar para a rede um vetor contendo as amostras de consumo de memória da aplicação e determinar quais são os BMU's dada a amostra.\\

Assim, uma aplicação que visita uma área mais avermelhada (pixels com mais vermelho do que outras cores), possui um consumo de memória maior do que a velocidade e a aceleração (vcm e tvcm, respectivamente), visto que possui uma porção maior de vermelho na composição da cor. Uma aplicação que visita uma área mais azulada, possui uma variação na aceleração maior do que as outras duas dimensões, e assim por diante. Com base nessas características e no programa desenvolvido em \cite{Alecrim:07}, é possível identificar as áreas visitadas do SOM dado um log de entrada, ou seja, dado um vetor de entrada com os valores da memória consumida da aplicação testada no decorrer do tempo. Espera-se com isso conseguir identificar qual aplicação está executando com base no padrão de comportamento de áreas visitadas do SOM, ou ainda definir o perfil que o consumo de memória irá seguir, dado o consumo instantâneo.\\

Nas Figuras \ref{fig:browser}, \ref{fig:browser-cc} e \ref{fig:browser-cc-tun}, temos a visualização das áreas visitadas do SOM de acordo com as amostras coletadas durante a execução do Browser, ou seja, quais foram os BMU's encontrados de acordo com o valor da memória consumida pelo Browser. Note que as áreas são diferentes dado um cenário (Alpha, Beta ou Gama). Isso confirma o fato de que o comportamento do consumo de memória do Browser é influenciado quando o Cache Comprimido está presente ou ainda quando o \textit{swappiness} é configurado com valor igual a $60$.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/browser.png}
	\caption{Cenário Alpha: regiões onde a aplicação 'Browser' visitou no SOM.}
	\label{fig:browser}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-browser.png}
	\caption{Cenário Beta: regiões onde a aplicação 'Browser' visitou no SOM.}
	\label{fig:browser-cc}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-tun-browser.png}
	\caption{Cenário Gama: regiões onde a aplicação 'Browser' visitou no SOM.}
	\label{fig:browser-cc-tun}
\end{figure}

Como visto nas Figuras \ref{fig:browser}, \ref{fig:browser-cc} e \ref{fig:browser-cc-tun}, nos cenários Alpha e Beta, o consumo de memória do Browser, mapeado no SOM de cada rede mostra que o mesmo é maior do que a velocidade e a aceleração, indicando um valor alto. No cenário Gama, quando o Cache Comprimido e o swappiness estavam configurados, o consumo de memória do Browser visitou mais áreas azuladas do SOM, indicando um consumo de memória mais baixo.\\

As áreas visitadas para o Canola e o PDF viewer são mostradas nas Figuras \ref{fig:canola}, \ref{fig:canola-cc}, \ref{fig:canola-cc-tun}, \ref{fig:pdf}, \ref{fig:pdf-cc} e \ref{fig:pdf-cc-tun}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/canola.png}
	\caption{Cenário Alpha: regiões onde o 'Canola' visitou no SOM.}
	\label{fig:canola}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-canola.png}
	\caption{Cenário Beta: regiões onde o 'Canola' visitou no SOM.}
	\label{fig:canola-cc}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-tun-canola.png}
	\caption{Cenário Gama: regiões onde o Canola visitou no SOM.}
	\label{fig:canola-cc-tun}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/pdf.png}
	\caption{Cenário Alpha: regiões onde o 'PDF viewer' visitou no SOM.}
	\label{fig:pdf}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-pdf.png}
	\caption{Cenário Beta: regiões onde o 'PDF viewer' visitou no SOM.}
	\label{fig:pdf-cc}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/cc-tun-pdf.png}
	\caption{Cenário Gama: regiões onde o 'PDF viewer' visitou no SOM.}
	\label{fig:pdf-cc-tun}
\end{figure}

Nas áreas visitadas pelo PDF e pelo Canola, não notamos uma diferença quando os cenários são mudados. Conclui-se que o Cache Comprimido não possui muita influência no consumo de memória dessas aplicações. Porém, no caso do Browser, houve um consumo mais baixo da memória quando o Cache Comprimido estava ativo e o \textit{swap virtual} configurado para receber as páginas de memória mais cedo (swappiness = 60). Isso pode ser visualizado no gráfico do cenário Gama, Figura \ref{fig:browser-cc-tun}.\\

Uma outra medida, não menos importante, é a frequência de visita dos pontos (visualizada aqui no retângulo central de cada figura). Esse gráfico mostra quais pontos foram mais visitados durante a execução da aplicação avaliada. A frequência é calculada pela quantidade de vezes que um BMU é encontrado quando comparado a um valor dos dados de entrada. Cada vez que um pixel é visitado, ele ganha 1 unidade de cor preta. Assim, pontos mais escuros indicam mais visitas.\\

De novo, podemos notar que quando o Cache Comprimido estava ligado, os pontos mais visitados estavam na área azulada do SOM, indicando um baixo consumo da memória principal do sistema.

Analisando as áreas do SOM, visitadas por cada aplicação testada, podemos chegar à conclusão de 3 perfis para o consumo de memória:

\begin{itemize}
 \item Alto consumo de memória: são aquelas aplicações que tiveram seus perfis visitando áreas no SOM em tons de vermelho.
 \item Baixo consumo de memória: são aquelas aplicações que tiveram seus perfis visitando áreas no SOM em tons de azul.
 \item Mediano consumo de memória: são aquelas aplicações que possuem seus perfis como uma mistura dos outros dois, não prevalecendo nenhuma área.
\end{itemize}

Dado cada perfil, é possível desenvolver um programa capaz de identificar qual aplicação está sendo executada ou em qual perfil de consumo ela se encaixa, com base na rede neural já treinada de acordo com o cenário utilizado: Alpha, Beta ou Gama. Esse programa é usado para tomar uma decisão: ligar ou não o Cache Comprimido e ainda, qual será a configuração do mesmo. Esse sistema é apresentado no capítulo a seguir.

\section{Sumário}

Neste Capítulo foram apresentados conceitos de redes neurais e mapas auto-organizáveis. Também foi apresentado um resumo dos trabalhos usados como referência \cite{Mlin:06, Alecrim:07}, os quais também demonstram resultados utilizando SOMs para classificar padrões de consumo de memória.\\

Neste Capítulo também foram mostradas as redes neurais e seus respectivos SOMs, treinadas para identificar o consumo de memória das aplicações testadas em diferentes cenários, entre eles, o cenário com o Cache Comprimido ligado. Com isso, foi observado que o Cache Comprimido exerce uma influência positiva no consumo de memória de aplicações que requerem uma quantidade grande de memória. Com o Cache Comprimido o comportamento do consumo de memória de tais aplicações cai, liberando assim mais memória para outras aplicações.

\chapter{Cache Comprimido Adaptativo}

Neste Capítulo será apresentada uma solução para implementar o Cache Comprimido Adaptativo, ou seja, a possibilidade do Cache Comprimido ser ligado ou desligado e ser configurado em tempo real de acordo com o perfil de consumo de memória atual e em um futuro próximo.\\

Os testes com o Cache Comprimido (CC) são executados em Linux embarcado na plataforma ARM OMAP. O equipamento não possui área de swap configurada e possui memória total igual a 128MB. Para mais detalhes sobre a configuração de hardware do dispositivo, ver seção  \ref{sec:metodologia}.

\section{Adaptatividade no Cache Comprimido}

Implementar a adaptatividade ao CC é adaptá-lo de acordo com o consumo de memória instantâneo e futuro. Para implementar essa nova característica alguns requisitos são necessários:

\begin{itemize}
 \item Ter uma amostra do consumo de memória durante um tempo pré-definido.
 \item Ter um padrão que confronte Configurações do CC versus Padrão de Consumo de memória. Esse padrão vale para decidir o que fazer quando determinado nível de memória é atingido.
 \item Implementar uma heurística para definir a configuração do CC que será adotada, de acordo com a amostra de memória.
\end{itemize}

Como visto na seção \ref{sec:classificando}, cada aplicação possui um SOM característico. Além disso, o SOM de cada aplicação testada depende do cenário adotado: se existe ou não CC ativo, por exemplo. O programa utilizado em \cite{Alecrim:07}, além de gerar o SOM para um log de consumo de memória, também gera uma matriz de frequências, como visto nas Figuras \ref{fig:browser-freq}, \ref{fig:browser-freq-beta}, \ref{fig:browser-freq-gama}, \ref{fig:canola-freq}, \ref{fig:canola-freq-beta}, \ref{fig:canola-freq-gama}, \ref{fig:pdf-freq}, \ref{fig:pdf-freq-beta} e \ref{fig:pdf-freq-gama} (área sombreada):

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/browser-freq.png}
	\caption{Cenário Alpha: imagem que representa o SOM para o Browser e sua matriz de frequências.}
	\label{fig:browser-freq}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/browser-freq-beta.png}
	\caption{Cenário Beta: imagem que representa o SOM para o Browser e sua matriz de frequências.}
	\label{fig:browser-freq-beta}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/browser-freq-gama.png}
	\caption{Cenário Gama: imagem que representa o SOM para o Browser e sua matriz de frequências.}
	\label{fig:browser-freq-gama}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/canola-freq.png}
	\caption{Cenário Alpha: imagem que representa o SOM para o media player Canola e sua matriz de frequências.}
	\label{fig:canola-freq}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/canola-freq-beta.png}
	\caption{Cenário Beta: imagem que representa o SOM para o media player Canola e sua matriz de frequências.}
	\label{fig:canola-freq-beta}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/canola-freq.png}
	\caption{Cenário Gama: imagem que representa o SOM para o media player Canola e sua matriz de frequências.}
	\label{fig:canola-freq-gama}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/pdf-freq.png}
	\caption{Cenário Alpha: imagem que representa o SOM para o visualizador de PDF's e sua matriz de frequências.}
	\label{fig:pdf-freq}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/pdf-freq-beta.png}
	\caption{Cenário Beta: imagem que representa o SOM para o visualizador de PDF's e sua matriz de frequências.}
	\label{fig:pdf-freq-beta}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{figs/pdf-freq-gama.png}
	\caption{Cenário Gama: imagem que representa o SOM para o visualizador de PDF's e sua matriz de frequências.}
	\label{fig:pdf-freq-gama}
\end{figure}

A matriz de frequência de cada aplicação mostra quantas vezes aquele BMU foi visitado. Quanto mais escuro é o pixel, mais vezes o BMU foi selecionado.\\

Se for possível determinar os BMUs em tempo de execução e compará-los com as matrizes de frequência das três aplicações testadas, é possível determinar qual aplicação está sendo executada e tomar medidas quanto ao Cache Comprimido assim que essa identificação é feita. O algoritmo seria o seguinte:

\begin{enumerate}
 \item Ler o saved\_som.txt contendo a rede neural treinada para o cenário dado (o código está disponível no Apêndice B). A rede neural é representada por uma matriz quadrática de ordem 40 em que cada célula representa um \textbf{som\_node} com os pesos sinápticos.
 \item Armazenar em matrizes de ordem 40 a tabela de frequência para cada aplicação: browser, pdf e canola.
 \item Realizar leituras periódicas da memória livre em um intervalo de tempo ($t = 1$s). Guardar os valores em um vetor de inteiros.
 \item Calcular o BMU para cada valor do vetor construído no item anterior. Comparar esse BMU com a matriz de frequências de cada aplicação.
\end{enumerate}


O algoritmo descrito foi implementado em um programa e testado na plataforma ARM, mais detalhes na seção \ref{sec:metodologia}. O código mostrado em \ref{alg:calc_profile}, escrito em linguagem C, ilustra o algoritmo apresentado:

\begin{program}
\begin{verbatim}
int calculate_profile() {
    struct rss_list *head = NULL;
    struct rss_list *iterator = NULL;
    int x, i, j;
    unsigned int current_rss = 0;
    unsigned int new_rss;
    struct som_node *bmu = NULL;
    int veloc = 0;
    int accel = 0;
    int browser_factor = 0;
    int canola_factor = 0;
    int pdf_factor = 0;

    i = j = 0;
    /* Collect memFree for x seconds */
    populate_mem_free_array();

    /* Get the head pointer for rss_list */
    head = read_mem_free_log();
    iterator = head;

    for (x = 0; x < MAX_INPUTS; x++) {
        new_rss = iterator->rss_pages;
        iterator = iterator->next;

        /* Get the closest bmu */
        bmu = get_bmu_xy(grids,
                         &new_rss,
                         &current_rss,
                         &veloc,
                         &accel);
            i = bmu->xp;
            j = bmu->yp;

            if (freq_pdf[i][j] != 0) {
                pdf_factor = pdf_factor + freq_pdf[i][j];
            } else if (freq_browser[i][j] != 0) {
                browser_factor = browser_factor + freq_browser[i][j];
            } else if (freq_canola[i][j] != 0) {
                canola_factor = canola_factor + freq_canola[i][j];
            }
    }
    if ((pdf_factor > browser_factor) && (pdf_factor > canola_factor)) {
        return ALPHA_PROFILE;
    } else if (browser_factor > canola_factor)
        return GAMA_PROFILE;
    else
        return BETA_PROFILE;
}
\end{verbatim}
\caption{Algoritmo usado para calcular o fator de semelhança entre os BMU's.}
\label{alg:calc_profile}
\end{program}

A variável \texttt{MAX\_INPUTS} possui o tamanho do vetor das amostras de memória livre em tempo real. O programa foi feito para realizar leituras no \texttt{/proc/sys/vm/memfree} a cada segundo. Assim, o programa possui uma latência de \texttt{MAX\_INPUTS} segundos, tirando o tempo de processamento, para calcular os \textbf{fatores de semelhança}. Esse fator de semelhança é a quantidade de vezes que o BMU calculado em tempo real é igual ao BMU de uma das 3 matrizes de frequências, dado um dos cenários Alpha, Beta ou Gama.\\

O objetivo dessa função é calcular um \textit{fator de semelhança}, ou seja, o maior valor entre \texttt{pdf\_factor}, \texttt{browser\_factor} e \texttt{canola\_factor}, indica ainda qual matriz de frequências teve mais BMUs em comum em relação à coleta de memória livre do sistema. Assim, o maior valor entre as 3 variáveis indica qual aplicação está sendo executada em um determinado momento.\\

Identificando qual aplicação está sendo executada, e conhecendo seu comportamento de consumo de memória, pode-se decidir qual configuração passar para o CC, em termos de tamanho, e ainda qual valor para o \texttt{swappiness} e \texttt{min\_free\_kbytes}, ambas variáveis de configurações do kernel do Linux. Isso é feito em tempo de execução.\\

\subsection{Daemon para adaptar o Cache Comprimido}

No Linux, programas do tipo \textit{daemon} são programas que possuem sua execução sendo feita em \textit{background}, ou seja, são programas que não esperam uma interação direta do usuário.\\

Para adaptar o CC ao consumo de memória, foi desenvolvido um \textit{daemon} que, dentre outras operações, utiliza o Algoritmo \ref{alg:calc_profile} para determinar qual perfil executar do CC. Esse \textit{daemon} também é responsável por decidir que configuração realizar no CC, ou ainda, quando ligá-lo ou desligá-lo. O Algoritmo \ref{alg:main} mostra a função \textit{main} do \textit{daemon}.

\begin{program}
\begin{scriptsize}
\begin{verbatim}
int main()
{
    /* Starts profile as ALPHA_PROFILE: CC is not configured */
    current_profile = ALPHA_PROFILE;

    do {
        if (current_profile != previous_profile) {
            if (current_profile == ALPHA_PROFILE) {
                printf("ALPHA PROFILE\n");
                system("sh unuse_compcache.sh");

                previous_profile = ALPHA_PROFILE;

                /* Save trained som in grids struct */
                read_trained_som(NO_CC_DIR"saved_som.txt", grids);

                init_freq_tables();

                read_freq_log(NO_CC_DIR"freq-pdf.log", freq_pdf);
                read_freq_log(NO_CC_DIR"freq-browser.log", freq_browser);
                read_freq_log(NO_CC_DIR"freq-canola.log", freq_canola);

                read_max_min_values(NO_CC_DIR"max_min_values.txt");
            } else if (current_profile == BETA_PROFILE) {
                printf("BETA PROFILE\n");
                system("sh unuse_compcache.sh");
                system("sh use_compcache.sh 5120");

                previous_profile = BETA_PROFILE;

                /* Save trained som in grids struct */
                read_trained_som(WITH_CC_DIR"saved_som.txt", grids);

                init_freq_tables();

                read_freq_log(WITH_CC_DIR"freq-pdf.log", freq_pdf);
                read_freq_log(WITH_CC_DIR"freq-browser.log", freq_browser);
                read_freq_log(WITH_CC_DIR"freq-canola.log", freq_canola);

                read_max_min_values(WITH_CC_DIR"max_min_values.txt");
                } else if (current_profile == GAMA_PROFILE) {
                    printf("GAMA PROFILE\n");
                    system("sh unuse_compcache.sh");
                    system("sh use_compcache.sh 10240");
                    system("echo 60 > /proc/sys/vm/swappiness");

                    previous_profile = GAMA_PROFILE;

                    /* Save trained som in grids struct */
                    read_trained_som(WITH_CC_DIR"saved_som.txt", grids);

                    init_freq_tables();

                    read_freq_log(WITH_CC_DIR"freq-pdf.log", freq_pdf);
                    read_freq_log(WITH_CC_DIR"freq-browser.log", freq_browser);
                    read_freq_log(WITH_CC_DIR"freq-canola.log", freq_canola);

                    read_max_min_values(WITH_CC_DIR"max_min_values.txt");

                    }
            }

                current_profile = calculate_profile();
        } while(1);

        return 0;
}
\end{verbatim}
\end{scriptsize}
\caption{Algoritmo usado no daemon de adaptatividade.}
\label{alg:main}
\end{program}

Note que cada perfil adotado executa ações diferentes no sistema. Para o perfil \texttt{ALPHA}, o CC é desligado e a área de swap virtual é retirada através do comando do Linux, \texttt{swapoff}. Para o perfil \texttt{BETA}, o CC é ligado com tamanho aproximado de 5MB, nesse caso espera-se um consumo mediano da memória. Para o perfil \texttt{GAMA}, o CC é ligado com tamanho aproximado de 10MB e \texttt{swappiness} $= 60$, o que faz as páginas irem mais cedo para o swap virtual onde são comprimidas e armazenadas. Nesse perfil esperamos um consumo intenso da memória ou a possível falta da mesma.\\

Cada perfil é encontrado usando a função apresentada no Algoritmo \ref{alg:calc_profile}. Esse algoritmo utiliza a rede neural treinada, as matrizes de frequência do BMU de cada aplicação (Browser, Canola e PDF viewer) e as coletas da memória livre durante 20 segundos (valor da variável MAX\_INPUTS). A cada 20 segundos, um vetor contendo 20 leituras da memória consumida é passado para a função apresentada no Algoritmo \ref{alg:calc_profile}. Essa função determina qual das 3 matrizes de frequência teve BMUs mais visitados. A matriz de frequência que teve mais BMUs visitados indica que aplicação está sendo executada naquele instante, ou ainda, qual é o perfil de consumo da memória.\\

O acerto em determinar o perfil de memória depende da quantidade de coletas que serão feitas antes da análise nas matrizes de frequências. Porém, coletar a memória leva, como dito anteriormente, \texttt{MAX\_INPUT} segundos, onde esse valor é o tamanho do vetor de coletas da memória livre. Assim, existe um ajuste para que o \textit{daemon} não leve muito tempo para determinar em que perfil está ou para qual perfil o consumo de memória está caminhando.\\

Na Figura \ref{fig:mem-adapt}, está representada a memória livre ao longo de um teste feito para checar se a adaptatividade do CC traz benefícios ou não. O teste consiste em abrir 8 instâncias do Browser, tocar um vídeo no Canola, simultaneamente e ainda abrir um arquivo PDF. De acordo com que a memória vai sendo consumida, os BMUs das leituras feitas em tempo real visitam mais as áreas avermelhadas do SOM treinado para o perfil (Alpha, Beta ou Gama) atual. A cada 20 segundos é feita a avaliação do consumo de memória e determinado o perfil do mesmo. De acordo com o perfil determinado, o CC é ligado ou desligado, o swappiness é configurado ou não. (mais detalhes, ver Algoritmo \ref{alg:main}. Se o padrão de consumo da memória se mantém constante, não há mudança de perfil.\\

Ainda na Figura \ref{fig:mem-adapt}, podemos notar que a memória livre (MemFree), varia bastante ao longo do tempo do teste. Também podemos notar que existem alguns picos de liberação da memória. Acreditamos que isso se deve ao fato de que quando o perfil Alpha é selecionado, o swap virtual é retirado da memória e acontece um aumento na memória livre. Também podemos notar que o uso do \textit{daemon} mantém uma média de memória livre maior do que em casos onde ele não é utilizado (Ver Figura \ref{fig:mem-consumption-all-apps}, para comparação).

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-adapt.pdf}
 \caption{Memória livre (MemFree) ao longo do teste com o CC adaptativo.}
 \label{fig:mem-adapt}
\end{figure}

\begin{figure}[htb]
 \centering
 \includegraphics[scale=0.6]{figs/mem-consumption-all-apps-no-cc.pdf}
 \caption{Memória livre (MemFree) para cada aplicação sem CC.}
 \label{fig:mem-consumption-all-apps}
\end{figure}

Na Figura \ref{fig:mem-consumption-all-apps}, está plotado um gráfico que mostra a variação na memória livre para cada uma das aplicações testadas. Os dados mostram como a memória é consumida ao longo do tempo quando o CC está desligado. Comparando com a Figura \ref{fig:mem-adapt}, pode-se notar que a memória média é maior quando o \textit{daemon} desenvolvido (chamado de adaptived), está sendo executado.

\section{Sumário}
Neste Capítulo foi apresentada uma heurística e a solução encontrada para utilizar redes neurais e perfis de consumo de memória na adaptatividade do Cache Comprimido. Também foi mostrado que um \textit{daemon} foi implementado para configurar o CC e a área de swap virtual afim de obter um uso mais eficiente da memória total do sistema.\\

Nos gráficos de consumo de memória das aplicações, notamos que o daemon deixa uma memória livre média maior do que quando o CC está desligado, indicando assim um consumo de memória mais otimizado.


\chapter{Conclusão e Trabalhos Futuros}
Neste Capítulo são apresentados algumas ideias para trabalhos futuros envolvendo o Cache Comprimido e a adaptatividade. Também é apresentada a conclusão deste trabalho de mestrado.



\section{Conclusão}

Neste trabalho de mestrado foi mostrada uma solução para otimização da memória livre no Linux através da compressão de dados. Os dados estão armazenados em páginas de memória e são comprimidos quando são enviados para uma área de swap virtual. Essa solução é chamada Cache Comprimido.\\

Nesta dissertação foram realizados vários testes com o Cache Comprimido. Todos os testes foram realizados em uma máquina real, ou seja, sem virtualização ou simulação. Essa máquina compreende um dispositivo móvel com Linux embarcado, 128MB de RAM e um processador ARM1136 com 400Mhz de \textit{clock}. O dispositivo móvel possui tela sensível ao toque e um conjunto de aplicações pré-instaladas. Essa aplicações são, na sua maioria, para acessar os serviços da Internet e também prover acesso ao conteúdo multimídia instalado.\\

Os testes com o Cache Comprimido visam identificar se é benéfico ou não o uso desse mecanismo no consumo de memória das aplicações e na performance das mesmas. Foi verificado que com o uso do Cache Comprimido, existe um aumento da memória livre disponível, resultando assim na possibilidade de abrir outras aplicações simultaneamente ou melhor a velocidade de respostas das aplicações que já estão em execução.\\

Porém, também foi verificado que em situações que o consumo de memória é altíssimo, a memória alocada para o Cache Comprimido se torna prejudicial e o ideal seria que seu tamanho fosse diminuído, ou ainda que existisse um mecanismo para liberar a memória consumida pelo próprio Cache Comprimido.\\

O Cache Comprimido é estático. Ou seja, seu tamanho não varia de acordo com o consumo de memória, ou ainda, de acordo com alguma heurística que busque a otimização da sua utilização. Foi mostrado que um Cache Comprimido de tamanho errado pode gerar mais situações de falta de memória, chamando um dispositivo do kernel chamado \textit{Out-of-memory killer} (OOM killer).\\

Após os testes com o Cache Comprimido, redes neurais foram treinadas para gerar os SOMs e assim encontrar um padrão de consumo para cada aplicação testada: Browser, Canola (\textit{media player}) e PDF \textit{viewer}. Essas aplicações foram escolhidas pois representam ações reais que um usuário comum poderia executar nesse tipo de dispositivo móvel.\\

Foi verificado um padrão de consumo para cada aplicação testada dentro de cada cenário: Alpha -- sem Cache Comprimido, Beta -- com Cache Comprimido e Gama -- Cache Comprimido e configuração extra para swap. Também mostramos que o consumo de cada aplicação é diferente dado um cenário.\\

Como proposto nesta dissertação, a classificação dos padrões de consumo de memória utilizando redes neurais e Mapas Auto-organizáveis (ou SOMs), desenvolvida em \cite{Alecrim:07} e \cite{Mlin:06}, é bastante útil para determinar padrões de consumo de memória e pré-determinar um comportamento nesse consumo. Utilizando essa ferramenta, foram determinados perfis de consumo de memória afim de implementar a adaptatividade do CC. Essa adaptatividade consiste em implementar um programa que identifique o padrão de consumo e configure o CC de acordo com esse padrão, tudo feito em tempo de execução. Utilizando os mapas gerados pela análise de cada aplicação, pode-se avaliar em quais momentos é necessário que o tamanho do CC seja alterado afim de suprir a necessidade de memória livre do sistema.\\
 
Com a criação de um mapa com casos de uso que simulem a utilização do sistema por um usuário final, é possível determinar quais são as áreas mais acessadas por determinadas aplicações. Nesse caso, é possível verificar qual é o comportamento de consumo de memória para um determinado caso de uso e assim, tomar medidas para que não falte memória. Essas medidas podem ser o aumento ou diminuição do tamanho do CC, a mudança para um algoritmo de compressão mais eficaz (porém mais lento), etc. A tripla $<$memória, vcm, tvcm$>$, pode ser utilizada para "prever" o consumo de memória baseado no consumo anterior e no perfil (mapa) da aplicação analisada. O parâmetro "tvcm" é especialmente importante pois pode ser utilizado como a "velocidade" que uma aplicação aloca mais memória. Muito útil para evitar que situações que o OOM \textit{killer} seja invocado.\\

A heurística utilizando SOMs e redes neurais, proposta para implementar a adaptatividade ao Cache Comprimido mostrou-se eficiente, aumentando a quantidade média de memória livre mesmo quando o consumo de memória é alto. Essa conclusão leva a acreditar que é possível executar as aplicações com mais memória disponível, ou ainda, executar um número maior de aplicações com a mesma quantidade de memória, sem a necessidade de alterações de \textit{hardware} (aumentando a capacidade da memória, por exemplo).

\section{Trabalhos Futuros}
Quanto ao Cache Comprimido, existem alguns pontos, apresentados em \cite{ccache09} de melhorias e implementação de novas características:

\begin{itemize}
 \item \textit{Swap free notification}: hoje, o swap virtual (ramzswap) não possui um mecanismo que libere as páginas que não são mais necessárias, como páginas de processos que foram finalizados.
 \item Implementar uma heurística que também contemple uma área de swap real, ou seja, quando o CC estiver sem capacidade de armazenas mais páginas, redirecioná-las para um swap real. No caso de sistemas embarcados essa pode não ser uma solução ótima pois esses sistemas geralmente são \textit{swapless} (sem área de swap real).
\end{itemize}

No daemon implementado com a adaptatividade, o SOM representando a rede neural treinada é carregado para cada perfil adotado. Isso é um problema se tivermos muitos perfis ou se usarmos uma rede neural com uma grade maior de neurônios (a grade atual tem tamanho $40 x 40$). O ideal seria ter um SOM para a rede neural treinada que conseguisse identificar os vários perfis adotados.\\

Uma característica ao daemon que pode ser implementada é a identificação dos perfis em um tempo menor. Assim o tempo de reação do daemon ao consumo de memória pode ser menor, evitando casos em que a memória atinja um nível mínimo entre uma leitura e outra. Hoje o tempo mínimo para identificar um perfil é de $25$ segundos, que é o tempo gasto para coletar as informações de memória livre. Esse tempo não está levando em conta o tempo de processamento para achar o BMU.\\

Um outro trabalho futuro que visa a otimização do consumo da memória em Linux embarcado é alterar o algoritmo de seleção das páginas de memória que vão para a área de swap (o PFRA, \textit{Page Frame Reclaiming Algorithm}). Implementando flags de identificação nas páginas afim de definir quais são comprimidas, seria possível tratá-las de forma diferente e adiar ou antecipar a compressão e descompressão, otimizando o overhead de leitura e escrita das páginas.\\

O Cache Comprimido usado neste trabalho faz uso da memória RAM do Linux. Existem soluções mais especializadas que fazem uso de uma memória em cache (L1 ou L2), ou ainda um chip especial para armazenar dados comprimidos, sem necessariamente utilizar a memória RAM.\\

\addcontentsline{toc}{section}{Referências}
%\bibliographystyle{alpha}
%\newpage

\begin{flushleft}
	\bibliography{briglia-ref}
	\bibliographystyle{plain}
\end{flushleft}

\input{apendice}
\input{anexo}

\end{document}
